[[34m2022-07-24 23:12:02,584[0m] {[34mscheduler_job.py:[0m708} INFO[0m - Starting the scheduler[0m
[[34m2022-07-24 23:12:02,585[0m] {[34mscheduler_job.py:[0m713} INFO[0m - Processing each file at most -1 times[0m
[[34m2022-07-24 23:12:02,590[0m] {[34mexecutor_loader.py:[0m105} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2022-07-24 23:12:02,594[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 12388[0m
[[34m2022-07-24 23:12:02,596[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:12:02,602[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-24 23:12:02,617] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-24 23:17:02,690[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:22:02,928[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:27:03,068[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:32:03,220[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:37:03,372[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:42:03,624[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:51:38,096[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 00:58:03,137[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 01:30:33,507[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 08:57:53,493[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 08:57:53,505[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-25 21:48:31,978[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 21:53:32,118[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 21:58:32,253[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 22:03:32,735[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 22:56:36,943[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 22:56:36,948[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-25 23:01:36,899[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:06:37,137[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:11:37,382[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:12:51,131[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:12:45.850485+00:00, run_after=2022-07-26T15:12:45.850485+00:00[0m
[[34m2022-07-25 23:12:51,175[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-24 15:12:45.850485+00:00: scheduled__2022-07-24T15:12:45.850485+00:00, externally triggered: False> successful[0m
[[34m2022-07-25 23:12:51,180[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-24 15:12:45.850485+00:00, run_id=scheduled__2022-07-24T15:12:45.850485+00:00, run_start_date=2022-07-25 15:12:51.144059+00:00, run_end_date=2022-07-25 15:12:51.180860+00:00, run_duration=0.036801, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-24 15:12:45.850485+00:00, data_interval_end=2022-07-25 15:12:45.850485+00:00, dag_hash=c9827c605aef0d340979e0977662107b[0m
[[34m2022-07-25 23:12:51,184[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:12:45.850485+00:00, run_after=2022-07-26T15:12:45.850485+00:00[0m
[[34m2022-07-25 23:16:37,569[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:21:37,745[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:26:37,949[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:27:50,647[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:27:50,650[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:27:50,651[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:27:50,663[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T15:27:49.983292+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-25 23:27:50,664[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:27:50,667[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:27:52,182[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:27:52,229[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:27:52,229[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:27:52,533[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:27:57,605[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T15:27:49.983292+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:28:18,040[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T15:27:49.983292+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:28:18,057[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T15:27:49.983292+00:00, map_index=-1, run_start_date=2022-07-25 15:28:07.645489+00:00, run_end_date=2022-07-25 15:28:17.798813+00:00, run_duration=10.153324, state=success, executor_state=success, try_number=1, max_tries=1, job_id=2, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 15:27:50.652908+00:00, queued_by_job_id=1, pid=23792[0m
[[34m2022-07-25 23:28:18,233[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:28:18,234[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:28:18,235[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:28:18,238[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T15:27:49.983292+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-25 23:28:18,239[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:28:18,240[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:28:19,419[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:28:19,452[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:28:19,453[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:28:19,684[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:28:24,757[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T15:27:49.983292+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:28:45,126[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T15:27:49.983292+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:28:45,136[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T15:27:49.983292+00:00, map_index=-1, run_start_date=2022-07-25 15:28:34.800605+00:00, run_end_date=2022-07-25 15:28:44.944355+00:00, run_duration=10.14375, state=success, executor_state=success, try_number=1, max_tries=1, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 15:28:18.237024+00:00, queued_by_job_id=1, pid=23801[0m
[[34m2022-07-25 23:28:45,288[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 15:27:49.983292+00:00: manual__2022-07-25T15:27:49.983292+00:00, externally triggered: True> successful[0m
[[34m2022-07-25 23:28:45,289[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 15:27:49.983292+00:00, run_id=manual__2022-07-25T15:27:49.983292+00:00, run_start_date=2022-07-25 15:27:50.591179+00:00, run_end_date=2022-07-25 15:28:45.289513+00:00, run_duration=54.698334, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 15:27:49.983292+00:00, data_interval_end=2022-07-25 15:27:49.983292+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-25 23:28:45,292[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:27:49.983292+00:00, run_after=2022-07-26T15:27:49.983292+00:00[0m
[[34m2022-07-25 23:31:38,248[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:36:38,435[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:41:38,653[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:46:38,842[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:51:39,019[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:52:13,854[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:13,856[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:52:13,857[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:13,860[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T15:52:12.586954+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-25 23:52:13,860[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:13,862[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:15,080[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:52:15,114[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:52:15,115[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:15,361[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:20,436[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T15:52:12.586954+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:52:40,861[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T15:52:12.586954+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:52:40,871[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T15:52:12.586954+00:00, map_index=-1, run_start_date=2022-07-25 15:52:30.481164+00:00, run_end_date=2022-07-25 15:52:40.632446+00:00, run_duration=10.151282, state=success, executor_state=success, try_number=1, max_tries=1, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 15:52:13.857980+00:00, queued_by_job_id=1, pid=26902[0m
[[34m2022-07-25 23:52:41,044[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:41,045[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:52:41,045[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:41,048[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T15:52:12.586954+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-25 23:52:41,048[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:41,050[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:42,270[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:52:42,308[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:52:42,309[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:42,548[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:47,625[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T15:52:12.586954+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:53:08,035[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T15:52:12.586954+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:53:08,044[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T15:52:12.586954+00:00, map_index=-1, run_start_date=2022-07-25 15:52:57.669107+00:00, run_end_date=2022-07-25 15:53:07.821243+00:00, run_duration=10.152136, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 15:52:41.046721+00:00, queued_by_job_id=1, pid=26914[0m
[[34m2022-07-25 23:53:08,192[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 15:52:12.586954+00:00: manual__2022-07-25T15:52:12.586954+00:00, externally triggered: True> successful[0m
[[34m2022-07-25 23:53:08,193[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 15:52:12.586954+00:00, run_id=manual__2022-07-25T15:52:12.586954+00:00, run_start_date=2022-07-25 15:52:13.831843+00:00, run_end_date=2022-07-25 15:53:08.193561+00:00, run_duration=54.361718, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 15:52:12.586954+00:00, data_interval_end=2022-07-25 15:52:12.586954+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-25 23:53:08,196[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:52:12.586954+00:00, run_after=2022-07-26T15:52:12.586954+00:00[0m
[[34m2022-07-25 23:56:39,219[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 00:00:39,764[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:00:39,766[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:00:39,767[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:00:39,770[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T16:00:01+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 00:00:39,770[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:00:39,772[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:00:41,089[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:00:41,135[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:00:41,135[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:00:41,382[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:00:46,475[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:01+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:01:06,873[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T16:00:01+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:01:06,882[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T16:00:01+00:00, map_index=-1, run_start_date=2022-07-25 16:00:56.516906+00:00, run_end_date=2022-07-25 16:01:06.678815+00:00, run_duration=10.161909, state=success, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 16:00:39.768037+00:00, queued_by_job_id=1, pid=27608[0m
[[34m2022-07-26 00:01:07,063[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:01:07,064[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:01:07,064[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 00:01:07,065[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:01:07,068[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T16:00:58.056951+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 00:01:07,068[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:07,069[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T16:00:01+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 00:01:07,069[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:07,071[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:08,271[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:01:08,323[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:01:08,323[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:08,560[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:13,645[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:58.056951+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:01:34,041[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:35,369[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:01:35,417[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:01:35,417[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:35,648[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:40,739[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:01+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:02:01,122[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T16:00:58.056951+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:02:01,124[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T16:00:01+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:02:01,139[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T16:00:01+00:00, map_index=-1, run_start_date=2022-07-25 16:01:50.781240+00:00, run_end_date=2022-07-25 16:02:00.936189+00:00, run_duration=10.154949, state=success, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 16:01:07.066432+00:00, queued_by_job_id=1, pid=27682[0m
[[34m2022-07-26 00:02:01,140[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T16:00:58.056951+00:00, map_index=-1, run_start_date=2022-07-25 16:01:23.684437+00:00, run_end_date=2022-07-25 16:01:33.860610+00:00, run_duration=10.176173, state=success, executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 16:01:07.066432+00:00, queued_by_job_id=1, pid=27616[0m
[[34m2022-07-26 00:02:01,151[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=12388) last sent a heartbeat 54.12 seconds ago! Restarting it[0m
[[34m2022-07-26 00:02:01,159[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 12388. PIDs of all processes in the group: [12388][0m
[[34m2022-07-26 00:02:01,160[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 12388[0m
[[34m2022-07-26 00:02:01,739[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=12388, status='terminated', exitcode=0, started='2022-07-24 23:12:02') (12388) terminated with exit code 0[0m
[[34m2022-07-26 00:02:01,746[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 27691[0m
[[34m2022-07-26 00:02:01,760[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-07-26 00:02:01,777[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 00:02:01,780[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2022-07-26 00:02:01,783] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-26 00:02:01,997[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 16:00:01+00:00: manual__2022-07-25T16:00:01+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 00:02:01,997[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 16:00:01+00:00, run_id=manual__2022-07-25T16:00:01+00:00, run_start_date=2022-07-25 16:00:39.741362+00:00, run_end_date=2022-07-25 16:02:01.997947+00:00, run_duration=82.256585, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 16:00:01+00:00, data_interval_end=2022-07-25 16:00:01+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-26 00:02:02,001[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T16:00:01+00:00, run_after=2022-07-26T16:00:01+00:00[0m
[[34m2022-07-26 00:02:02,013[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>[0m
[[34m2022-07-26 00:02:02,013[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:02:02,014[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>[0m
[[34m2022-07-26 00:02:02,016[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T16:00:58.056951+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 00:02:02,017[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:02:02,018[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:02:03,253[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:02:03,290[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:02:03,290[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:02:03,541[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:02:08,655[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:58.056951+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:02:29,035[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T16:00:58.056951+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:02:29,045[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T16:00:58.056951+00:00, map_index=-1, run_start_date=2022-07-25 16:02:18.697098+00:00, run_end_date=2022-07-25 16:02:28.854638+00:00, run_duration=10.15754, state=success, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 16:02:02.015070+00:00, queued_by_job_id=1, pid=27709[0m
[[34m2022-07-26 00:02:29,193[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 16:00:58.056951+00:00: manual__2022-07-25T16:00:58.056951+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 00:02:29,194[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 16:00:58.056951+00:00, run_id=manual__2022-07-25T16:00:58.056951+00:00, run_start_date=2022-07-25 16:01:07.038383+00:00, run_end_date=2022-07-25 16:02:29.194174+00:00, run_duration=82.155791, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 16:00:58.056951+00:00, data_interval_end=2022-07-25 16:00:58.056951+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-26 00:02:29,196[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T16:00:58.056951+00:00, run_after=2022-07-26T16:00:58.056951+00:00[0m
[[34m2022-07-26 00:03:28,191[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:28,193[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:03:28,193[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:28,197[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T16:02:53+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 00:03:28,198[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:28,199[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:29,574[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:03:29,642[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:03:29,642[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:03:29,893[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:03:34,975[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T16:02:53+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:03:55,369[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T16:02:53+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:03:55,378[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T16:02:53+00:00, map_index=-1, run_start_date=2022-07-25 16:03:45.018125+00:00, run_end_date=2022-07-25 16:03:55.176607+00:00, run_duration=10.158482, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 16:03:28.194910+00:00, queued_by_job_id=1, pid=27837[0m
[[34m2022-07-26 00:03:55,547[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:55,547[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:03:55,548[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:55,550[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T16:02:53+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 00:03:55,551[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:55,552[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:56,464[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:03:56,493[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:03:56,494[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:03:56,695[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:04:01,773[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T16:02:53+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:04:22,161[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T16:02:53+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:04:22,172[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T16:02:53+00:00, map_index=-1, run_start_date=2022-07-25 16:04:11.816277+00:00, run_end_date=2022-07-25 16:04:21.967684+00:00, run_duration=10.151407, state=success, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 16:03:55.549349+00:00, queued_by_job_id=1, pid=27846[0m
[[34m2022-07-26 00:04:22,328[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 16:02:53+00:00: manual__2022-07-25T16:02:53+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 00:04:22,329[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 16:02:53+00:00, run_id=manual__2022-07-25T16:02:53+00:00, run_start_date=2022-07-25 16:03:28.170645+00:00, run_end_date=2022-07-25 16:04:22.329246+00:00, run_duration=54.158601, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 16:02:53+00:00, data_interval_end=2022-07-25 16:02:53+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-26 00:04:22,331[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T16:02:53+00:00, run_after=2022-07-26T16:02:53+00:00[0m
[[34m2022-07-26 00:07:11,364[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 00:59:22,404[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 01:59:34,468[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:02:20,285[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:07:20,428[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:12:20,743[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:17:20,888[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:22:21,032[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:27:21,200[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:32:21,300[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:34:42,463[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:34:42,464[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 04:34:42,465[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 04:34:42,465[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:34:42,468[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 04:34:42,469[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:34:42,469[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 04:34:42,470[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:34:42,472[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:34:43,761[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:34:43,816[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:34:43,817[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:34:44,079[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:34:49,162[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:35:09,630[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:10,843[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:35:10,895[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:35:10,896[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:11,182[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:16,294[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:35:36,681[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:35:36,683[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:35:36,693[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:34:59.198130+00:00, run_end_date=2022-07-25 20:35:09.397227+00:00, run_duration=10.199097, state=success, executor_state=success, try_number=1, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-25 20:34:42.466541+00:00, queued_by_job_id=1, pid=32943[0m
[[34m2022-07-26 04:35:36,694[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:35:26.340540+00:00, run_end_date=2022-07-25 20:35:36.505597+00:00, run_duration=10.165057, state=success, executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 20:34:42.466541+00:00, queued_by_job_id=1, pid=32948[0m
[[34m2022-07-26 04:35:36,705[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=27691) last sent a heartbeat 54.29 seconds ago! Restarting it[0m
[[34m2022-07-26 04:35:36,715[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 27691. PIDs of all processes in the group: [27691][0m
[[34m2022-07-26 04:35:36,717[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 27691[0m
[[34m2022-07-26 04:35:37,096[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=27691, status='terminated', exitcode=0, started='00:02:01') (27691) terminated with exit code 0[0m
[[34m2022-07-26 04:35:37,102[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 32949[0m
[[34m2022-07-26 04:35:37,115[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-26 04:35:37,142] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-26 04:35:37,360[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:35:37,361[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 04:35:37,361[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 04:35:37,362[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:35:37,364[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 04:35:37,365[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:37,366[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 04:35:37,366[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:37,368[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:38,558[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:35:38,597[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:35:38,597[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:38,830[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:43,914[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:36:14,242[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:36:15,460[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:36:15,511[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:36:15,512[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:36:15,747[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:36:20,830[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:36:41,246[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:36:41,247[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:36:41,255[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:35:53.955741+00:00, run_end_date=2022-07-25 20:36:09.171016+00:00, run_duration=15.215275, state=success, executor_state=success, try_number=1, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-25 20:35:37.362955+00:00, queued_by_job_id=1, pid=32954[0m
[[34m2022-07-26 04:36:41,256[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:36:30.870856+00:00, run_end_date=2022-07-25 20:36:41.024363+00:00, run_duration=10.153507, state=success, executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 20:35:37.362955+00:00, queued_by_job_id=1, pid=33038[0m
[[34m2022-07-26 04:36:41,267[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=32949) last sent a heartbeat 63.94 seconds ago! Restarting it[0m
[[34m2022-07-26 04:36:41,275[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 32949. PIDs of all processes in the group: [32949][0m
[[34m2022-07-26 04:36:41,275[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 32949[0m
[[34m2022-07-26 04:36:41,572[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=32949, status='terminated', exitcode=0, started='04:35:37') (32949) terminated with exit code 0[0m
[[34m2022-07-26 04:36:41,577[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 33039[0m
[[34m2022-07-26 04:36:41,585[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-26 04:36:41,605] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-26 04:36:41,789[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 20:34:41.637079+00:00: manual__2022-07-25T20:34:41.637079+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 04:36:41,790[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 20:34:41.637079+00:00, run_id=manual__2022-07-25T20:34:41.637079+00:00, run_start_date=2022-07-25 20:34:42.438960+00:00, run_end_date=2022-07-25 20:36:41.790469+00:00, run_duration=119.351509, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 20:34:41.637079+00:00, data_interval_end=2022-07-25 20:34:41.637079+00:00, dag_hash=1b9c2d33dcbd23233be2491803b3e455[0m
[[34m2022-07-26 04:36:41,793[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T20:34:41.637079+00:00, run_after=2022-07-26T20:34:41.637079+00:00[0m
[[34m2022-07-26 04:37:21,473[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:42:21,576[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:47:21,726[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:52:21,865[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:57:22,017[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:02:22,159[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:07:22,301[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:12:22,445[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:17:22,596[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:22:22,680[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:27:22,935[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:32:23,180[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:37:23,324[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 06:29:54,281[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 07:45:12,305[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:12:13,117[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:17:13,254[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:22:13,391[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:27:13,468[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:32:13,564[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:37:13,808[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:42:14,058[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:47:14,306[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:52:14,322[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 14:57:14,467[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:02:14,625[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:07:14,689[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:12:14,833[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:17:15,031[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:22:15,177[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:27:15,323[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:32:15,472[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:37:15,633[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:42:15,787[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:47:16,030[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:52:16,063[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 15:57:16,207[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 16:02:16,365[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 17:33:00,186[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 17:33:00,191[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-26 23:06:36,070[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 23:21:15,230[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-26T15:12:45.850485+00:00, run_after=2022-07-27T15:12:45.850485+00:00[0m
[[34m2022-07-26 23:21:15,287[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-26 23:21:15,288[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 23:21:15,289[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 23:21:15,289[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-26 23:21:15,292[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-25T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 23:21:15,293[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 23:21:15,294[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-25T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 23:21:15,294[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 23:21:15,296[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 23:21:17,310[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 23:21:17,422[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 23:21:17,423[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 23:21:17,748[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 23:36:18,931[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-25T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 00:06:35,723[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 00:06:37,221[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 00:06:37,281[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 00:06:37,282[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 00:06:37,527[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 00:06:42,601[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 00:36:42,405[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-07-27 00:36:42,425[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-25T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-27 00:36:42,429[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-25T15:12:45.850485+00:00 exited with status failed for try_number 1[0m
[[34m2022-07-27 00:36:42,485[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-25T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-26 15:51:24.229631+00:00, run_end_date=2022-07-26 16:06:31.051798+00:00, run_duration=906.822167, state=success, executor_state=success, try_number=1, max_tries=1, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-26 15:21:15.290366+00:00, queued_by_job_id=1, pid=49050[0m
[[34m2022-07-27 00:36:42,486[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-25T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-26 16:21:40.638744+00:00, run_end_date=None, run_duration=None, state=running, executor_state=failed, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-26 15:21:15.290366+00:00, queued_by_job_id=1, pid=49100[0m
[[34m2022-07-27 00:36:42,500[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=33039) last sent a heartbeat 74.02 seconds ago! Restarting it[0m
[[34m2022-07-27 00:36:42,518[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 33039. PIDs of all processes in the group: [33039][0m
[[34m2022-07-27 00:36:42,520[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 33039[0m
[[34m2022-07-27 00:36:43,546[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=33039, status='terminated', exitcode=0, started='04:36:41') (33039) terminated with exit code 0[0m
[[34m2022-07-27 00:36:43,560[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 49120[0m
[[34m2022-07-27 00:36:43,589[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-07-27 00:36:43,612[0m] {[34mscheduler_job.py:[0m1362} WARNING[0m - Failing (1) jobs without heartbeat after 2022-07-26 16:31:43.603096+00:00[0m
[[34m2022-07-27 00:36:43,614[0m] {[34mscheduler_job.py:[0m1370} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/pro/Documents/proj_data/airflow/dags/test.py', 'msg': 'Detected <TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [running]> as zombie', 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff86511e410>, 'is_failure_callback': True}[0m
[2022-07-27 00:36:43,637] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-27 00:36:44,114[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 00:36:44,114[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-27 00:36:44,115[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 00:36:44,118[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-25T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-27 00:36:44,118[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 00:36:44,120[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 00:36:46,315[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 00:36:46,442[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 00:36:46,442[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 00:36:46,805[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 00:36:51,944[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 01:06:53,518[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-07-27 01:06:53,529[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-25T15:12:45.850485+00:00 exited with status failed for try_number 1[0m
[[34m2022-07-27 01:06:53,567[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-25T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-26 16:51:52.446137+00:00, run_end_date=None, run_duration=None, state=running, executor_state=failed, try_number=1, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-26 16:36:44.116061+00:00, queued_by_job_id=1, pid=None[0m
[[34m2022-07-27 01:06:53,604[0m] {[34mscheduler_job.py:[0m1362} WARNING[0m - Failing (1) jobs without heartbeat after 2022-07-26 17:01:53.600565+00:00[0m
[[34m2022-07-27 01:06:53,606[0m] {[34mscheduler_job.py:[0m1370} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/pro/Documents/proj_data/airflow/dags/test.py', 'msg': 'Detected <TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [running]> as zombie', 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff86299ea90>, 'is_failure_callback': True}[0m
[[34m2022-07-27 01:06:54,084[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 01:06:54,085[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-27 01:06:54,085[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 01:06:54,088[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-25T15:12:45.850485+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-27 01:06:54,088[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 01:06:54,090[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 01:06:55,832[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 01:06:55,904[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 01:06:55,904[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 01:06:56,377[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 01:07:01,568[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 02:42:42,820[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-07-27 02:42:42,827[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-25T15:12:45.850485+00:00 exited with status failed for try_number 2[0m
[[34m2022-07-27 02:42:42,859[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-25T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-26 17:48:31.658148+00:00, run_end_date=None, run_duration=903.284319, state=running, executor_state=failed, try_number=2, max_tries=1, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-26 17:06:54.086381+00:00, queued_by_job_id=1, pid=None[0m
[[34m2022-07-27 02:42:42,908[0m] {[34mscheduler_job.py:[0m1362} WARNING[0m - Failing (1) jobs without heartbeat after 2022-07-26 18:37:42.902571+00:00[0m
[[34m2022-07-27 02:42:42,909[0m] {[34mscheduler_job.py:[0m1370} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/pro/Documents/proj_data/airflow/dags/test.py', 'msg': 'Detected <TaskInstance: a_test.test_say_hello scheduled__2022-07-25T15:12:45.850485+00:00 [running]> as zombie', 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff86299ea90>, 'is_failure_callback': True}[0m
[[34m2022-07-27 02:42:43,645[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 02:42:43,646[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-27 02:42:43,647[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 02:42:43,650[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-25T15:12:45.850485+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-27 02:42:43,651[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 02:42:43,652[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-25T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 02:42:46,474[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 02:42:46,535[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 02:42:46,536[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 02:42:47,039[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 22:50:50,098[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-25T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 22:51:20,950[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-25T15:12:45.850485+00:00 exited with status success for try_number 2[0m
[[34m2022-07-27 22:51:20,977[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-25T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-27 14:51:00.583557+00:00, run_end_date=2022-07-27 14:51:15.806577+00:00, run_duration=15.22302, state=success, executor_state=success, try_number=2, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-26 18:42:43.648458+00:00, queued_by_job_id=1, pid=49273[0m
[[34m2022-07-27 22:51:21,341[0m] {[34mdagrun.py:[0m549} ERROR[0m - Marking run <DagRun a_test @ 2022-07-25 15:12:45.850485+00:00: scheduled__2022-07-25T15:12:45.850485+00:00, externally triggered: False> failed[0m
[[34m2022-07-27 22:51:21,342[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 15:12:45.850485+00:00, run_id=scheduled__2022-07-25T15:12:45.850485+00:00, run_start_date=2022-07-26 15:21:15.247087+00:00, run_end_date=2022-07-27 14:51:21.342085+00:00, run_duration=84606.094998, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-25 15:12:45.850485+00:00, data_interval_end=2022-07-26 15:12:45.850485+00:00, dag_hash=356dca66e0ec1749e066ef3dfe86f3e8[0m
[[34m2022-07-27 22:51:21,347[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-26T15:12:45.850485+00:00, run_after=2022-07-27T15:12:45.850485+00:00[0m
[[34m2022-07-27 22:52:53,595[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 22:57:53,845[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:02:54,087[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:07:54,236[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:12:46,341[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-27T15:12:45.850485+00:00, run_after=2022-07-28T15:12:45.850485+00:00[0m
[[34m2022-07-27 23:12:46,373[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 23:12:46,374[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-27 23:12:46,375[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-27 23:12:46,375[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 23:12:46,378[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-26T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-27 23:12:46,379[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:12:46,379[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-26T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-27 23:12:46,380[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:12:46,382[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:12:47,827[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 23:12:47,878[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 23:12:47,879[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:12:48,128[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:12:53,197[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-26T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 23:13:13,712[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:13:14,920[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 23:13:14,973[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 23:13:14,974[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:13:15,203[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:13:20,289[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-26T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 23:13:40,722[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-26T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-27 23:13:40,724[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-26T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-27 23:13:40,733[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-26T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-27 15:13:03.242606+00:00, run_end_date=2022-07-27 15:13:13.439839+00:00, run_duration=10.197233, state=success, executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-27 15:12:46.376631+00:00, queued_by_job_id=1, pid=50980[0m
[[34m2022-07-27 23:13:40,734[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-26T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-27 15:13:30.332756+00:00, run_end_date=2022-07-27 15:13:40.497480+00:00, run_duration=10.164724, state=success, executor_state=success, try_number=1, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-27 15:12:46.376631+00:00, queued_by_job_id=1, pid=50988[0m
[[34m2022-07-27 23:13:40,746[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=49120) last sent a heartbeat 54.44 seconds ago! Restarting it[0m
[[34m2022-07-27 23:13:40,756[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 49120. PIDs of all processes in the group: [49120][0m
[[34m2022-07-27 23:13:40,756[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 49120[0m
[[34m2022-07-27 23:13:41,218[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=49120, status='terminated', exitcode=0, started='00:36:43') (49120) terminated with exit code 0[0m
[[34m2022-07-27 23:13:41,225[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 50993[0m
[[34m2022-07-27 23:13:41,251[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-07-27 23:13:41,260[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:13:41,263[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2022-07-27 23:13:41,288] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-27 23:13:41,511[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 23:13:41,512[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-27 23:13:41,513[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-27 23:13:41,513[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-26T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-27 23:13:41,516[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-26T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-27 23:13:41,517[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:13:41,517[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-07-26T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-27 23:13:41,518[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:13:41,520[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:13:42,757[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 23:13:42,794[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 23:13:42,794[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:13:43,048[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:13:48,143[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-26T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 23:14:18,562[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-26T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-27 23:14:19,995[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-27 23:14:20,051[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-27 23:14:20,052[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:14:20,363[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-27 23:14:25,476[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-07-26T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-27 23:14:45,882[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-26T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-27 23:14:45,883[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-07-26T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-27 23:14:45,902[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-26T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-27 15:13:58.182572+00:00, run_end_date=2022-07-27 15:14:13.379963+00:00, run_duration=15.197391, state=success, executor_state=success, try_number=1, max_tries=3, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-27 15:13:41.514472+00:00, queued_by_job_id=1, pid=51001[0m
[[34m2022-07-27 23:14:45,904[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-07-26T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-27 15:14:35.522465+00:00, run_end_date=2022-07-27 15:14:45.683766+00:00, run_duration=10.161301, state=success, executor_state=success, try_number=1, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-27 15:13:41.514472+00:00, queued_by_job_id=1, pid=51009[0m
[[34m2022-07-27 23:14:45,916[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=50993) last sent a heartbeat 64.44 seconds ago! Restarting it[0m
[[34m2022-07-27 23:14:45,923[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 50993. PIDs of all processes in the group: [50993][0m
[[34m2022-07-27 23:14:45,924[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 50993[0m
[[34m2022-07-27 23:14:46,342[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=50993, status='terminated', exitcode=0, started='23:13:41') (50993) terminated with exit code 0[0m
[[34m2022-07-27 23:14:46,347[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 51010[0m
[[34m2022-07-27 23:14:46,365[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-27 23:14:46,394] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-27 23:14:46,586[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-26 15:12:45.850485+00:00: scheduled__2022-07-26T15:12:45.850485+00:00, externally triggered: False> successful[0m
[[34m2022-07-27 23:14:46,588[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-26 15:12:45.850485+00:00, run_id=scheduled__2022-07-26T15:12:45.850485+00:00, run_start_date=2022-07-27 15:12:46.351244+00:00, run_end_date=2022-07-27 15:14:46.588291+00:00, run_duration=120.237047, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-26 15:12:45.850485+00:00, data_interval_end=2022-07-27 15:12:45.850485+00:00, dag_hash=a3e103954599917a23502fa63e272933[0m
[[34m2022-07-27 23:14:46,592[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-27T15:12:45.850485+00:00, run_after=2022-07-28T15:12:45.850485+00:00[0m
[[34m2022-07-27 23:18:41,399[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:23:41,637[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:28:41,714[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:33:41,777[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:38:41,922[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:43:42,071[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:48:42,215[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:53:42,458[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-27 23:58:42,712[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 00:03:42,968[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 00:08:43,118[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 00:13:43,264[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 01:28:55,672[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 01:28:55,677[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-28 02:15:00,236[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 05:45:23,562[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 06:32:22,690[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 06:37:23,016[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 06:42:23,146[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 06:47:23,159[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 06:52:23,288[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 06:57:23,432[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 07:02:23,564[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 07:07:23,694[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 07:12:23,945[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 07:17:23,951[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 08:00:14,343[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 14:00:07,494[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 14:00:07,502[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-28 15:52:03,896[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 15:57:03,942[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 17:47:00,371[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 17:52:00,520[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 17:57:00,670[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:02:00,909[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:07:01,067[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:12:01,085[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:17:01,242[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:22:01,295[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:27:01,449[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:32:01,601[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:37:01,873[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 18:42:01,929[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-28 23:44:34,014[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-28T15:12:45.850485+00:00, run_after=2022-07-29T15:12:45.850485+00:00[0m
[[34m2022-07-28 23:44:34,086[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-28 23:44:34,087[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-28 23:44:34,088[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-28 23:44:34,089[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-28 23:44:34,094[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-27T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-28 23:44:34,095[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-28 23:44:34,096[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-27T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-28 23:44:34,096[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-28 23:44:34,098[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-28 23:44:36,300[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-28 23:44:36,345[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-28 23:44:36,345[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-28 23:44:36,762[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-28 23:44:41,885[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-29 03:44:46,070[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-07-29 03:44:46,093[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 03:44:48,530[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-29 03:44:48,578[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-29 03:44:48,578[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 03:44:49,119[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 03:44:54,328[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-27T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-29 06:43:31,269[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-27T15:12:45.850485+00:00 exited with status failed for try_number 1[0m
[[34m2022-07-29 06:43:31,277[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-27T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-29 06:43:31,340[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-27T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-28 16:44:44.842473+00:00, run_end_date=None, run_duration=None, state=running, executor_state=failed, try_number=1, max_tries=1, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-28 15:44:34.090748+00:00, queued_by_job_id=1, pid=None[0m
[[34m2022-07-29 06:43:31,342[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-27T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-28 20:44:55.510514+00:00, run_end_date=2022-07-28 22:43:30.924400+00:00, run_duration=7115.413886, state=success, executor_state=success, try_number=1, max_tries=1, job_id=26, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-28 15:44:34.090748+00:00, queued_by_job_id=1, pid=86182[0m
[[34m2022-07-29 06:43:31,356[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=51010) last sent a heartbeat 65.84 seconds ago! Restarting it[0m
[[34m2022-07-29 06:43:31,370[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 51010. PIDs of all processes in the group: [51010][0m
[[34m2022-07-29 06:43:31,373[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 51010[0m
[[34m2022-07-29 06:43:31,840[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=51010, status='terminated', exitcode=0, started='2022-07-27 23:14:46') (51010) terminated with exit code 0[0m
[[34m2022-07-29 06:43:31,853[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 86198[0m
[[34m2022-07-29 06:43:31,886[0m] {[34mscheduler_job.py:[0m1362} WARNING[0m - Failing (1) jobs without heartbeat after 2022-07-28 22:38:31.879087+00:00[0m
[[34m2022-07-29 06:43:31,888[0m] {[34mscheduler_job.py:[0m1370} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/pro/Documents/proj_data/airflow/dags/test.py', 'msg': 'Detected <TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [running]> as zombie', 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff8652497d0>, 'is_failure_callback': True}[0m
[[34m2022-07-29 06:43:31,881[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-29 06:43:31,922] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-29 06:43:32,259[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-29 06:43:32,259[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-29 06:43:32,260[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-29 06:43:32,263[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-07-27T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-29 06:43:32,264[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 06:43:32,266[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 06:43:33,501[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-29 06:43:33,531[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-29 06:43:33,532[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 06:43:33,779[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 08:07:31,507[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-07-27T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-29 08:07:51,624[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-07-27T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-29 08:07:51,687[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-07-27T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-29 00:07:41.673886+00:00, run_end_date=2022-07-29 00:07:51.132506+00:00, run_duration=9.45862, state=success, executor_state=success, try_number=1, max_tries=1, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-28 22:43:32.261544+00:00, queued_by_job_id=1, pid=86242[0m
[[34m2022-07-29 08:07:52,303[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-29 08:07:52,304[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-29 08:07:52,305[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-29 08:07:52,310[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-27T15:12:45.850485+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-29 08:07:52,311[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 08:07:52,313[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 08:07:54,034[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-29 08:07:54,075[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-29 08:07:54,075[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 08:07:54,368[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 08:07:59,496[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-27T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-29 08:08:20,083[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-27T15:12:45.850485+00:00 exited with status success for try_number 2[0m
[[34m2022-07-29 08:08:20,093[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-27T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-29 00:08:09.566138+00:00, run_end_date=2022-07-29 00:08:19.803084+00:00, run_duration=10.236946, state=success, executor_state=success, try_number=2, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-29 00:07:52.307045+00:00, queued_by_job_id=1, pid=86332[0m
[[34m2022-07-29 08:08:20,113[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 08:08:20,368[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-29 08:08:20,369[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-29 08:08:20,369[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-27T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-29 08:08:20,372[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-27T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-29 08:08:20,372[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 08:08:20,374[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-27T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-29 08:08:21,586[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-29 08:08:21,635[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-29 08:08:21,635[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 08:08:21,875[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-29 08:08:26,966[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-27T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-29 08:08:57,489[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-27T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-29 08:08:57,517[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-27T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-29 00:08:37.028349+00:00, run_end_date=2022-07-29 00:08:52.323115+00:00, run_duration=15.294766, state=success, executor_state=success, try_number=1, max_tries=3, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-29 00:08:20.370544+00:00, queued_by_job_id=1, pid=86344[0m
[[34m2022-07-29 08:08:57,826[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-27 15:12:45.850485+00:00: scheduled__2022-07-27T15:12:45.850485+00:00, externally triggered: False> successful[0m
[[34m2022-07-29 08:08:57,826[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-27 15:12:45.850485+00:00, run_id=scheduled__2022-07-27T15:12:45.850485+00:00, run_start_date=2022-07-28 15:44:34.032561+00:00, run_end_date=2022-07-29 00:08:57.826709+00:00, run_duration=30263.794148, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-27 15:12:45.850485+00:00, data_interval_end=2022-07-28 15:12:45.850485+00:00, dag_hash=b2fe1d431fc455238911dc31b974081f[0m
[[34m2022-07-29 08:08:57,831[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-28T15:12:45.850485+00:00, run_after=2022-07-29T15:12:45.850485+00:00[0m
[[34m2022-07-29 08:13:20,259[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 08:18:20,476[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 08:23:20,777[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 08:28:20,919[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 08:33:21,064[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 09:22:22,532[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 09:27:22,779[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 09:32:22,807[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 09:37:22,840[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 09:42:22,984[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 09:47:23,145[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 10:37:09,273[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-29 10:37:09,277[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-29 15:34:46,339[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 05:28:36,589[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-29T21:28:36.588268+00:00, run_after=2022-07-30T21:28:36.588268+00:00[0m
[[34m2022-07-31 05:28:36,883[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-31 05:28:36,925[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-31 05:28:36,926[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-31 05:28:36,931[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-31 05:28:36,944[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-28T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-31 05:28:36,945[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:28:36,946[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-28T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-31 05:28:36,947[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:28:36,950[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:28:40,564[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:28:40,599[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:28:40,599[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:28:41,068[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:28:46,272[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-28T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:34:35,913[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:34:37,424[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:34:37,467[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:34:37,467[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:34:37,704[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:34:42,800[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-28T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:35:07,005[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-28T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:35:07,018[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-28T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:35:07,098[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-28T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-30 21:28:50.765279+00:00, run_end_date=2022-07-30 21:34:35.737557+00:00, run_duration=344.972278, state=success, executor_state=success, try_number=1, max_tries=1, job_id=30, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-30 21:28:36.933789+00:00, queued_by_job_id=1, pid=91603[0m
[[34m2022-07-31 05:35:07,104[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-28T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-30 21:34:53.135688+00:00, run_end_date=2022-07-30 21:35:05.582358+00:00, run_duration=12.44667, state=success, executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-30 21:28:36.933789+00:00, queued_by_job_id=1, pid=91640[0m
[[34m2022-07-31 05:35:07,141[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=86198) last sent a heartbeat 61.46 seconds ago! Restarting it[0m
[[34m2022-07-31 05:35:07,222[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 86198. PIDs of all processes in the group: [86198][0m
[[34m2022-07-31 05:35:07,228[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 86198[0m
[[34m2022-07-31 05:35:11,259[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=86198, status='terminated', exitcode=0, started='2022-07-29 06:43:31') (86198) terminated with exit code 0[0m
[[34m2022-07-31 05:35:11,305[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 91646[0m
[[34m2022-07-31 05:35:11,419[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-31 05:35:11,655] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-31 05:35:14,097[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-31 05:35:14,103[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-31 05:35:14,108[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-31 05:35:14,112[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-28T15:12:45.850485+00:00 [scheduled]>[0m
[[34m2022-07-31 05:35:14,154[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-28T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-31 05:35:14,172[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:35:14,178[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-07-28T15:12:45.850485+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-31 05:35:14,186[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:35:14,197[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:35:27,337[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:35:27,763[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:35:27,765[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:35:29,948[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:35:35,819[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-28T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:36:16,000[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-28T15:12:45.850485+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:36:17,288[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:36:17,356[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:36:17,357[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:36:17,627[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:36:22,851[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-07-28T15:12:45.850485+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:36:44,608[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-28T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:36:44,613[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-07-28T15:12:45.850485+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:36:44,632[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-28T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-30 21:35:46.048864+00:00, run_end_date=2022-07-30 21:36:01.837907+00:00, run_duration=15.789043, state=success, executor_state=success, try_number=1, max_tries=3, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-30 21:35:14.120359+00:00, queued_by_job_id=1, pid=91659[0m
[[34m2022-07-31 05:36:44,633[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-07-28T15:12:45.850485+00:00, map_index=-1, run_start_date=2022-07-30 21:36:33.399309+00:00, run_end_date=2022-07-30 21:36:44.246423+00:00, run_duration=10.847114, state=success, executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-30 21:35:14.120359+00:00, queued_by_job_id=1, pid=91680[0m
[[34m2022-07-31 05:36:44,647[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=91646) last sent a heartbeat 90.84 seconds ago! Restarting it[0m
[[34m2022-07-31 05:36:44,678[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 91646. PIDs of all processes in the group: [91646][0m
[[34m2022-07-31 05:36:44,679[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 91646[0m
[[34m2022-07-31 05:36:45,060[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=91646, status='terminated', exitcode=0, started='05:35:11') (91646) terminated with exit code 0[0m
[[34m2022-07-31 05:36:45,068[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 91681[0m
[[34m2022-07-31 05:36:45,085[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-07-31 05:36:45,108[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[2022-07-31 05:36:45,108] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-31 05:36:45,112[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-31 05:36:45,462[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-28 15:12:45.850485+00:00: scheduled__2022-07-28T15:12:45.850485+00:00, externally triggered: False> successful[0m
[[34m2022-07-31 05:36:45,463[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-28 15:12:45.850485+00:00, run_id=scheduled__2022-07-28T15:12:45.850485+00:00, run_start_date=2022-07-30 21:28:36.650911+00:00, run_end_date=2022-07-30 21:36:45.463584+00:00, run_duration=488.812673, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-28 15:12:45.850485+00:00, data_interval_end=2022-07-29 15:12:45.850485+00:00, dag_hash=473eb777c03fda3a0cc4ebba55ba6b11[0m
[[34m2022-07-31 05:36:45,469[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-29T21:36:45.469235+00:00, run_after=2022-07-30T21:36:45.469235+00:00[0m
[[34m2022-07-31 05:36:46,614[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-30T21:36:45.469235+00:00, run_after=2022-07-31T21:36:45.469235+00:00[0m
[[34m2022-07-31 05:36:46,653[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-07-31 05:36:46,654[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-31 05:36:46,655[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-31 05:36:46,655[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-07-31 05:36:46,659[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-29T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-31 05:36:46,659[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:36:46,660[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-29T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-31 05:36:46,661[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:36:46,663[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:36:52,684[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:36:52,782[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:36:52,783[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:36:53,259[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:36:59,026[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-29T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:37:19,760[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:37:21,248[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:37:21,315[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:37:21,315[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:37:21,608[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:37:26,701[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-29T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:37:48,231[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-29T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:37:48,237[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-29T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:37:48,270[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-29T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-07-30 21:37:09.187482+00:00, run_end_date=2022-07-30 21:37:19.466011+00:00, run_duration=10.278529, state=success, executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-30 21:36:46.656712+00:00, queued_by_job_id=1, pid=91697[0m
[[34m2022-07-31 05:37:48,271[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-29T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-07-30 21:37:37.483726+00:00, run_end_date=2022-07-30 21:37:47.761228+00:00, run_duration=10.277502, state=success, executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-30 21:36:46.656712+00:00, queued_by_job_id=1, pid=91709[0m
[[34m2022-07-31 05:37:48,284[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=91681) last sent a heartbeat 61.68 seconds ago! Restarting it[0m
[[34m2022-07-31 05:37:48,295[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 91681. PIDs of all processes in the group: [91681][0m
[[34m2022-07-31 05:37:48,296[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 91681[0m
[[34m2022-07-31 05:37:48,761[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=91681, status='terminated', exitcode=0, started='05:36:45') (91681) terminated with exit code 0[0m
[[34m2022-07-31 05:37:48,771[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 91714[0m
[[34m2022-07-31 05:37:48,786[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-31 05:37:48,812] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-31 05:37:49,186[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-07-31 05:37:49,187[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-31 05:37:49,188[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-31 05:37:49,189[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-29T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-07-31 05:37:49,192[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-29T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-31 05:37:49,193[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:37:49,194[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-07-29T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-31 05:37:49,195[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:37:49,197[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:37:54,808[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:37:54,876[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:37:54,877[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:37:55,402[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:38:00,603[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-29T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:38:31,726[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-29T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-31 05:38:33,059[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-31 05:38:33,114[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-31 05:38:33,114[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:38:33,347[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-31 05:38:38,443[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-07-29T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-31 05:38:58,985[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-29T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:38:58,988[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-07-29T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-07-31 05:38:59,013[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-29T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-07-30 21:38:11.278413+00:00, run_end_date=2022-07-30 21:38:26.806684+00:00, run_duration=15.528271, state=success, executor_state=success, try_number=1, max_tries=3, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-30 21:37:49.190437+00:00, queued_by_job_id=1, pid=91728[0m
[[34m2022-07-31 05:38:59,014[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-07-29T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-07-30 21:38:48.582394+00:00, run_end_date=2022-07-30 21:38:58.751598+00:00, run_duration=10.169204, state=success, executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-30 21:37:49.190437+00:00, queued_by_job_id=1, pid=91800[0m
[[34m2022-07-31 05:38:59,026[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=91714) last sent a heartbeat 69.88 seconds ago! Restarting it[0m
[[34m2022-07-31 05:38:59,034[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 91714. PIDs of all processes in the group: [91714][0m
[[34m2022-07-31 05:38:59,034[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 91714[0m
[[34m2022-07-31 05:38:59,372[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=91714, status='terminated', exitcode=0, started='05:37:48') (91714) terminated with exit code 0[0m
[[34m2022-07-31 05:38:59,380[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 91808[0m
[[34m2022-07-31 05:38:59,395[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-31 05:38:59,419] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-31 05:38:59,640[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-29 21:36:45.469235+00:00: scheduled__2022-07-29T21:36:45.469235+00:00, externally triggered: False> successful[0m
[[34m2022-07-31 05:38:59,641[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-29 21:36:45.469235+00:00, run_id=scheduled__2022-07-29T21:36:45.469235+00:00, run_start_date=2022-07-30 21:36:46.625005+00:00, run_end_date=2022-07-30 21:38:59.641818+00:00, run_duration=133.016813, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-29 21:36:45.469235+00:00, data_interval_end=2022-07-30 21:36:45.469235+00:00, dag_hash=473eb777c03fda3a0cc4ebba55ba6b11[0m
[[34m2022-07-31 05:38:59,646[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-30T21:36:45.469235+00:00, run_after=2022-07-31T21:36:45.469235+00:00[0m
[[34m2022-07-31 05:41:45,124[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 05:46:45,266[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 05:51:45,408[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 05:56:45,759[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:01:45,897[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:06:46,042[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:11:46,145[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:16:46,290[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:21:46,430[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:26:46,476[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:31:46,720[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:36:46,968[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:41:47,114[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:46:47,252[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:51:47,427[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 06:56:47,698[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 07:01:47,838[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 07:06:47,979[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 07:11:48,131[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 07:16:48,279[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 07:36:46,880[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 08:31:32,359[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-31 09:16:29,268[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-01 06:00:01,041[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-31T21:36:45.469235+00:00, run_after=2022-08-01T21:36:45.469235+00:00[0m
[[34m2022-08-01 06:00:01,081[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-08-01 06:00:01,082[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-01 06:00:01,082[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-01 06:00:01,083[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-08-01 06:00:01,088[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-07-30T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-01 06:00:01,090[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 06:00:01,091[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-07-30T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-01 06:00:01,091[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 06:00:01,095[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 06:00:03,087[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-01 06:00:03,315[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-01 06:00:03,316[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-01 06:00:04,168[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-01 06:00:09,456[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-07-30T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-01 11:00:11,499[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 11:00:12,690[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-01 11:00:12,736[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-01 11:00:12,737[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-01 11:00:12,967[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-01 12:00:05,435[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-07-30T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-01 18:00:11,923[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-07-30T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-08-01 18:00:11,931[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-07-30T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-08-01 18:00:11,986[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-07-30T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-08-01 00:55:35.217139+00:00, run_end_date=2022-08-01 03:00:10.695202+00:00, run_duration=7475.478063, state=success, executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-31 22:00:01.085387+00:00, queued_by_job_id=1, pid=99936[0m
[[34m2022-08-01 18:00:11,989[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-07-30T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-08-01 04:00:15.723579+00:00, run_end_date=2022-08-01 10:00:11.621140+00:00, run_duration=21595.897561, state=success, executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-31 22:00:01.085387+00:00, queued_by_job_id=1, pid=99975[0m
[[34m2022-08-01 18:00:12,002[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=91808) last sent a heartbeat 84.74 seconds ago! Restarting it[0m
[[34m2022-08-01 18:00:12,016[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 91808. PIDs of all processes in the group: [91808][0m
[[34m2022-08-01 18:00:12,019[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 91808[0m
[[34m2022-08-01 18:00:12,522[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=91808, status='terminated', exitcode=0, started='2022-07-31 05:38:59') (91808) terminated with exit code 0[0m
[[34m2022-08-01 18:00:12,534[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 99992[0m
[[34m2022-08-01 18:00:12,562[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-01 18:00:12,602] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-01 18:00:12,934[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-08-01 18:00:12,935[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-01 18:00:12,937[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-01 18:00:12,938[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-07-30T21:36:45.469235+00:00 [scheduled]>[0m
[[34m2022-08-01 18:00:12,945[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-07-30T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-01 18:00:12,946[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 18:00:12,946[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-07-30T21:36:45.469235+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-01 18:00:12,947[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 18:00:12,949[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-01 18:00:14,112[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-01 18:00:14,144[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-01 18:00:14,144[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-01 18:00:14,378[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-01 19:00:06,107[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-07-30T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-02 00:00:07,093[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-07-30T21:36:45.469235+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 00:00:08,353[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-02 00:00:08,403[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-02 00:00:08,403[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 00:00:08,635[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 11:00:05,252[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-07-30T21:36:45.469235+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-02 12:00:11,709[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-07-30T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-08-02 12:00:11,715[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-07-30T21:36:45.469235+00:00 exited with status success for try_number 1[0m
[[34m2022-08-02 12:00:11,763[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-07-30T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-08-01 12:45:05.702943+00:00, run_end_date=2022-08-01 16:00:06.821830+00:00, run_duration=11701.118887, state=success, executor_state=success, try_number=1, max_tries=3, job_id=40, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-01 10:00:12.941915+00:00, queued_by_job_id=1, pid=147[0m
[[34m2022-08-02 12:00:11,765[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-07-30T21:36:45.469235+00:00, map_index=-1, run_start_date=2022-08-02 03:00:15.394404+00:00, run_end_date=2022-08-02 04:00:11.139720+00:00, run_duration=3595.745316, state=success, executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-01 10:00:12.941915+00:00, queued_by_job_id=1, pid=257[0m
[[34m2022-08-02 12:00:11,778[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=99992) last sent a heartbeat 69.61 seconds ago! Restarting it[0m
[[34m2022-08-02 12:00:11,787[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 99992. PIDs of all processes in the group: [99992][0m
[[34m2022-08-02 12:00:11,788[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 99992[0m
[[34m2022-08-02 12:00:12,249[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=99992, status='terminated', exitcode=0, started='18:00:12') (99992) terminated with exit code 0[0m
[[34m2022-08-02 12:00:12,256[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 278[0m
[[34m2022-08-02 12:00:12,275[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-08-02 12:00:12,295[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 12:00:12,300[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2022-08-02 12:00:12,306] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-02 12:00:12,645[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-30 21:36:45.469235+00:00: scheduled__2022-07-30T21:36:45.469235+00:00, externally triggered: False> successful[0m
[[34m2022-08-02 12:00:12,646[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-30 21:36:45.469235+00:00, run_id=scheduled__2022-07-30T21:36:45.469235+00:00, run_start_date=2022-07-31 22:00:01.055502+00:00, run_end_date=2022-08-02 04:00:12.646286+00:00, run_duration=108011.590784, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-30 21:36:45.469235+00:00, data_interval_end=2022-07-31 21:36:45.469235+00:00, dag_hash=855a52f35030718a9c8fc75aebf7b68e[0m
[[34m2022-08-02 12:00:12,650[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-01T04:00:12.650445+00:00, run_after=2022-08-02T04:00:12.650445+00:00[0m
[[34m2022-08-02 12:00:13,788[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-02T04:00:12.650445+00:00, run_after=2022-08-03T04:00:12.650445+00:00[0m
[[34m2022-08-02 12:00:13,819[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-02 12:00:13,820[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-02 12:00:13,820[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-02 12:00:13,821[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-02 12:00:13,824[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-01T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-02 12:00:13,825[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 12:00:13,825[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-01T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-02 12:00:13,826[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 12:00:13,828[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 12:00:15,353[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-02 12:00:15,390[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-02 12:00:15,390[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 12:00:15,640[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 14:15:05,770[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-01T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-02 15:56:25,381[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 15:56:26,652[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-02 15:56:26,714[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-02 15:56:26,714[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 15:56:26,967[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 15:56:32,045[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-01T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-02 15:56:55,310[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-01T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-02 15:56:55,327[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-01T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-02 15:56:55,568[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-01T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-02 06:15:15.933339+00:00, run_end_date=2022-08-02 07:56:25.123800+00:00, run_duration=6069.190461, state=success, executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-02 04:00:13.822191+00:00, queued_by_job_id=1, pid=319[0m
[[34m2022-08-02 15:56:55,575[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-01T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-02 07:56:42.326310+00:00, run_end_date=2022-08-02 07:56:54.001077+00:00, run_duration=11.674767, state=success, executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-02 04:00:13.822191+00:00, queued_by_job_id=1, pid=368[0m
[[34m2022-08-02 15:56:55,603[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=278) last sent a heartbeat 57.87 seconds ago! Restarting it[0m
[[34m2022-08-02 15:56:55,666[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 278. PIDs of all processes in the group: [278][0m
[[34m2022-08-02 15:56:55,671[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 278[0m
[[34m2022-08-02 15:57:00,756[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=278, status='terminated', exitcode=0, started='12:00:12') (278) terminated with exit code 0[0m
[[34m2022-08-02 15:57:00,915[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 376[0m
[[34m2022-08-02 15:57:01,282[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-02 15:57:02,290] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-02 15:57:06,293[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-02 15:57:06,300[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-02 15:57:06,305[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-02 15:57:06,310[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-01T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-02 15:57:06,365[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-01T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-02 15:57:06,375[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 15:57:06,384[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-01T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-02 15:57:06,394[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 15:57:06,461[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 15:57:22,893[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-02 15:57:23,153[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-02 15:57:23,154[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 15:57:24,350[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 15:58:16,281[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-01T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-02 15:58:42,854[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-01T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-02 15:58:53,393[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-02 15:58:53,763[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-02 15:58:53,766[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 15:58:55,653[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-02 15:59:01,375[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-01T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-02 16:00:50,270[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-01T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-02 16:00:50,282[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-01T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-02 16:00:50,352[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-01T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-02 07:58:26.329758+00:00, run_end_date=2022-08-02 07:58:41.217669+00:00, run_duration=14.887911, state=success, executor_state=success, try_number=1, max_tries=3, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-02 07:57:06.331055+00:00, queued_by_job_id=1, pid=444[0m
[[34m2022-08-02 16:00:50,358[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-01T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-02 08:00:33.800175+00:00, run_end_date=2022-08-02 08:00:44.970582+00:00, run_duration=11.170407, state=success, executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-02 07:57:06.331055+00:00, queued_by_job_id=1, pid=496[0m
[[34m2022-08-02 16:00:50,382[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=376) last sent a heartbeat 99.01 seconds ago! Restarting it[0m
[[34m2022-08-02 16:00:50,552[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 376. PIDs of all processes in the group: [376][0m
[[34m2022-08-02 16:00:50,560[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 376[0m
[[34m2022-08-02 16:00:53,744[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=376, status='terminated', exitcode=0, started='15:57:00') (376) terminated with exit code 0[0m
[[34m2022-08-02 16:00:53,795[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 506[0m
[[34m2022-08-02 16:00:53,954[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-02 16:00:54,147] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-02 16:00:55,828[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-01 04:00:12.650445+00:00: scheduled__2022-08-01T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-02 16:00:55,836[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-01 04:00:12.650445+00:00, run_id=scheduled__2022-08-01T04:00:12.650445+00:00, run_start_date=2022-08-02 04:00:13.797563+00:00, run_end_date=2022-08-02 08:00:55.836362+00:00, run_duration=14442.038799, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-01 04:00:12.650445+00:00, data_interval_end=2022-08-02 04:00:12.650445+00:00, dag_hash=855a52f35030718a9c8fc75aebf7b68e[0m
[[34m2022-08-02 16:00:55,870[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-02T04:00:12.650445+00:00, run_after=2022-08-03T04:00:12.650445+00:00[0m
[[34m2022-08-02 16:03:02,014[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:08:02,273[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:30:21,636[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:35:21,788[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:40:21,929[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:45:22,144[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:50:22,259[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 16:55:22,507[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 17:00:22,664[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 17:05:22,819[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 17:10:23,067[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 17:15:23,323[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 17:20:23,354[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 18:13:51,088[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 18:13:51,094[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-08-02 21:11:48,795[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:16:48,954[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:21:49,199[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:26:49,218[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:31:49,372[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:36:49,486[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:41:49,635[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:46:49,780[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:51:50,028[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 21:56:50,171[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:01:50,321[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:06:50,469[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:11:50,644[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:16:50,790[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:21:50,954[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:26:51,211[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:31:51,391[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 22:36:51,533[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-02 23:27:07,250[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 00:26:44,746[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 01:20:12,961[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 03:04:40,755[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 14:09:37,124[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-03T04:00:12.650445+00:00, run_after=2022-08-04T04:00:12.650445+00:00[0m
[[34m2022-08-03 14:09:37,203[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-03 14:09:37,203[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-03 14:09:37,206[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-03 14:09:37,207[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-03 14:09:37,239[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-02T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-03 14:09:37,240[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:09:37,240[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-02T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-03 14:09:37,241[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:09:37,244[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:09:39,160[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-03 14:09:39,210[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-03 14:09:39,210[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:09:39,586[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:09:44,734[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-02T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-03 14:38:16,935[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:38:18,199[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-03 14:38:18,284[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-03 14:38:18,292[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:38:18,575[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:38:23,657[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-02T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-03 14:38:44,088[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-02T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-03 14:38:44,091[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-02T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-03 14:38:44,116[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-02T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-03 06:38:06.376837+00:00, run_end_date=2022-08-03 06:38:16.621558+00:00, run_duration=10.244721, state=success, executor_state=success, try_number=1, max_tries=1, job_id=46, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-03 06:09:37.208369+00:00, queued_by_job_id=1, pid=17279[0m
[[34m2022-08-03 14:38:44,117[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-02T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-03 06:38:33.702504+00:00, run_end_date=2022-08-03 06:38:43.860024+00:00, run_duration=10.15752, state=success, executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-03 06:09:37.208369+00:00, queued_by_job_id=1, pid=17303[0m
[[34m2022-08-03 14:38:44,130[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=506) last sent a heartbeat 55.87 seconds ago! Restarting it[0m
[[34m2022-08-03 14:38:44,143[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 506. PIDs of all processes in the group: [506][0m
[[34m2022-08-03 14:38:44,144[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 506[0m
[[34m2022-08-03 14:38:44,609[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=506, status='terminated', exitcode=0, started='16:00:53') (506) terminated with exit code 0[0m
[[34m2022-08-03 14:38:44,622[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 17304[0m
[[34m2022-08-03 14:38:44,645[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-03 14:38:44,682] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-03 14:38:45,033[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-03 14:38:45,036[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-03 14:38:45,036[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-03 14:38:45,037[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-02T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-03 14:38:45,041[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-02T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-03 14:38:45,042[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:38:45,043[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-02T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-03 14:38:45,044[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:38:45,045[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:38:46,248[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-03 14:38:46,285[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-03 14:38:46,286[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:38:46,522[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:38:51,608[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-02T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-03 14:39:19,614[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-02T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-03 14:39:20,898[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-03 14:39:20,951[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-03 14:39:20,952[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:39:21,211[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-03 14:39:26,313[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-02T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-03 14:39:46,707[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-02T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-03 14:39:46,709[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-02T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-03 14:39:46,718[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-02T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-03 06:39:01.651134+00:00, run_end_date=2022-08-03 06:39:14.524516+00:00, run_duration=12.873382, state=success, executor_state=success, try_number=1, max_tries=3, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-03 06:38:45.038205+00:00, queued_by_job_id=1, pid=17309[0m
[[34m2022-08-03 14:39:46,719[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-02T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-03 06:39:36.358963+00:00, run_end_date=2022-08-03 06:39:46.517059+00:00, run_duration=10.158096, state=success, executor_state=success, try_number=1, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-03 06:38:45.038205+00:00, queued_by_job_id=1, pid=17364[0m
[[34m2022-08-03 14:39:46,731[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=17304) last sent a heartbeat 64.06 seconds ago! Restarting it[0m
[[34m2022-08-03 14:39:46,737[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 17304. PIDs of all processes in the group: [17304][0m
[[34m2022-08-03 14:39:46,738[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 17304[0m
[[34m2022-08-03 14:39:47,039[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=17304, status='terminated', exitcode=0, started='14:38:44') (17304) terminated with exit code 0[0m
[[34m2022-08-03 14:39:47,045[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 17366[0m
[[34m2022-08-03 14:39:47,054[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-03 14:39:47,074] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-03 14:39:47,285[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-02 04:00:12.650445+00:00: scheduled__2022-08-02T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-03 14:39:47,285[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-02 04:00:12.650445+00:00, run_id=scheduled__2022-08-02T04:00:12.650445+00:00, run_start_date=2022-08-03 06:09:37.145036+00:00, run_end_date=2022-08-03 06:39:47.285718+00:00, run_duration=1810.140682, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-02 04:00:12.650445+00:00, data_interval_end=2022-08-03 04:00:12.650445+00:00, dag_hash=4450ce6edee20c6b2aac99803e083d6d[0m
[[34m2022-08-03 14:39:47,290[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-03T04:00:12.650445+00:00, run_after=2022-08-04T04:00:12.650445+00:00[0m
[[34m2022-08-03 14:40:02,848[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 14:45:02,980[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 14:58:40,393[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:03:40,516[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:08:40,646[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:13:40,774[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:18:40,866[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:23:40,990[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:28:43,349[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:33:43,487[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:38:43,710[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:43:43,839[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:48:43,946[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:53:44,184[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 15:58:44,309[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:03:44,441[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:08:44,675[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:13:44,818[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:18:44,957[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:23:45,182[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:28:45,343[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:33:45,498[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:38:45,703[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:43:45,970[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:48:46,240[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 16:53:46,334[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 21:37:41,052[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 21:42:41,288[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 21:47:41,431[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 21:52:41,543[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 21:57:41,652[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:02:41,786[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:07:41,848[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:12:41,923[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:17:42,029[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:22:42,180[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:27:42,429[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:32:42,566[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:37:42,831[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:42:42,981[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:47:43,222[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:52:43,579[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 22:57:43,724[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 23:02:43,874[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 23:07:44,128[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 23:12:44,410[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-03 23:54:24,452[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 00:54:17,437[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 02:39:22,216[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 03:18:06,938[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 04:34:40,068[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 05:03:45,624[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 05:08:45,874[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 05:13:46,029[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 05:18:46,184[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 05:23:46,343[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 06:10:27,915[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 12:00:35,293[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-04T04:00:12.650445+00:00, run_after=2022-08-05T04:00:12.650445+00:00[0m
[[34m2022-08-04 12:00:35,336[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-04 12:00:35,337[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-04 12:00:35,337[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-04 12:00:35,339[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-04 12:00:35,343[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-03T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-04 12:00:35,344[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 12:00:35,345[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-03T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-04 12:00:35,345[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 12:00:35,347[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 12:00:37,831[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-04 12:00:37,880[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-04 12:00:37,881[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 12:00:38,294[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 12:00:43,466[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-03T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-04 14:56:30,174[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 14:56:31,672[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-04 14:56:31,758[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-04 14:56:31,758[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 14:56:32,050[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 14:56:37,135[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-03T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-04 14:56:57,599[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-03T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-04 14:56:57,602[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-03T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-04 14:56:57,627[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-03T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-04 06:56:19.433529+00:00, run_end_date=2022-08-04 06:56:29.924687+00:00, run_duration=10.491158, state=success, executor_state=success, try_number=1, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-04 04:00:35.339987+00:00, queued_by_job_id=1, pid=52667[0m
[[34m2022-08-04 14:56:57,628[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-03T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-04 06:56:47.181682+00:00, run_end_date=2022-08-04 06:56:57.340203+00:00, run_duration=10.158521, state=success, executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-04 04:00:35.339987+00:00, queued_by_job_id=1, pid=52714[0m
[[34m2022-08-04 14:56:57,640[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=17366) last sent a heartbeat 58.83 seconds ago! Restarting it[0m
[[34m2022-08-04 14:56:57,673[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 17366. PIDs of all processes in the group: [17366][0m
[[34m2022-08-04 14:56:57,679[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 17366[0m
[[34m2022-08-04 14:56:58,211[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=17366, status='terminated', exitcode=0, started='2022-08-03 14:39:47') (17366) terminated with exit code 0[0m
[[34m2022-08-04 14:56:58,223[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 52715[0m
[[34m2022-08-04 14:56:58,251[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-04 14:56:58,286] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-04 14:56:58,526[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-04 14:56:58,527[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-04 14:56:58,528[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-04 14:56:58,528[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-04 14:56:58,532[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-03T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-04 14:56:58,533[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 14:56:58,533[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-03T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-04 14:56:58,534[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 14:56:58,536[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 14:56:59,751[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-04 14:56:59,788[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-04 14:56:59,788[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 14:57:00,027[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 14:57:05,117[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-04 15:11:36,938[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-08-04 15:11:36,939[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 15:11:38,375[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-04 15:11:38,425[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-04 15:11:38,426[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 15:11:38,807[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 15:11:43,964[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-03T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-04 15:41:44,611[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-03T04:00:12.650445+00:00 exited with status failed for try_number 1[0m
[[34m2022-08-04 15:41:44,613[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-03T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-04 15:41:44,624[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-03T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-04 06:57:15.157554+00:00, run_end_date=None, run_duration=None, state=running, executor_state=failed, try_number=1, max_tries=3, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-04 06:56:58.530127+00:00, queued_by_job_id=1, pid=None[0m
[[34m2022-08-04 15:41:44,625[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-03T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-04 07:26:44.427402+00:00, run_end_date=2022-08-04 07:41:43.992982+00:00, run_duration=899.56558, state=success, executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-04 06:56:58.530127+00:00, queued_by_job_id=1, pid=52758[0m
[[34m2022-08-04 15:41:44,639[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=52715) last sent a heartbeat 55.85 seconds ago! Restarting it[0m
[[34m2022-08-04 15:41:44,649[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 52715. PIDs of all processes in the group: [52715][0m
[[34m2022-08-04 15:41:44,649[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 52715[0m
[[34m2022-08-04 15:41:45,199[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=52715, status='terminated', exitcode=0, started='14:56:58') (52715) terminated with exit code 0[0m
[[34m2022-08-04 15:41:45,206[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 52788[0m
[[34m2022-08-04 15:41:45,216[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-04 15:41:45,233] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-04 15:41:45,234[0m] {[34mscheduler_job.py:[0m1362} WARNING[0m - Failing (1) jobs without heartbeat after 2022-08-04 07:36:45.230392+00:00[0m
[[34m2022-08-04 15:41:45,236[0m] {[34mscheduler_job.py:[0m1370} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/pro/Documents/proj_data/airflow/dags/test.py', 'msg': 'Detected <TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [running]> as zombie', 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff865237350>, 'is_failure_callback': True}[0m
[[34m2022-08-04 15:41:45,242[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 15:41:45,246[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-08-04 15:41:45,253[0m] {[34mscheduler_job.py:[0m1300} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [running]>[0m
[[34m2022-08-04 15:56:39,300[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-04 15:56:39,301[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-04 15:56:39,302[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-04 15:56:39,312[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-03T04:00:12.650445+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-04 15:56:39,312[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 15:56:39,319[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-03T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-04 15:56:41,261[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-04 15:56:41,304[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-04 15:56:41,304[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 15:56:41,643[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-04 15:56:46,794[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-03T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-04 16:00:07,960[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-03T04:00:12.650445+00:00 exited with status success for try_number 2[0m
[[34m2022-08-04 16:00:07,970[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-03T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-04 07:59:47.575730+00:00, run_end_date=2022-08-04 08:00:02.773094+00:00, run_duration=15.197364, state=success, executor_state=success, try_number=2, max_tries=3, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-04 07:56:39.309965+00:00, queued_by_job_id=1, pid=52834[0m
[[34m2022-08-04 16:00:08,238[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-03 04:00:12.650445+00:00: scheduled__2022-08-03T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-04 16:00:08,239[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-03 04:00:12.650445+00:00, run_id=scheduled__2022-08-03T04:00:12.650445+00:00, run_start_date=2022-08-04 04:00:35.307710+00:00, run_end_date=2022-08-04 08:00:08.239429+00:00, run_duration=14372.931719, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-03 04:00:12.650445+00:00, data_interval_end=2022-08-04 04:00:12.650445+00:00, dag_hash=1ef2c74be64c5697b7fe7475704f8a56[0m
[[34m2022-08-04 16:00:08,243[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-04T04:00:12.650445+00:00, run_after=2022-08-05T04:00:12.650445+00:00[0m
[[34m2022-08-04 16:04:27,894[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:09:28,084[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:14:28,360[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:19:28,474[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:24:28,737[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:29:28,874[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:34:29,055[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:39:29,319[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:44:29,592[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:49:29,877[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:54:30,138[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 16:59:30,298[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 17:04:30,425[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 17:09:30,639[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 17:14:30,711[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 17:19:30,682[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 17:24:30,826[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 17:29:31,048[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 21:48:06,088[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 21:53:06,228[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 21:58:06,468[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:03:06,500[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:08:06,642[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:13:06,789[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:18:06,930[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:23:07,071[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:28:07,214[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:33:07,373[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:38:07,389[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:43:07,418[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:48:07,667[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:53:07,821[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 22:58:07,956[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:03:08,125[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:08:08,292[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:13:08,431[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:18:08,719[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:23:08,865[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:50:32,558[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-04 23:50:32,564[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-08-05 06:55:41,881[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 08:15:01,674[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 13:13:23,964[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-05T04:00:12.650445+00:00, run_after=2022-08-06T04:00:12.650445+00:00[0m
[[34m2022-08-05 13:13:24,022[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-05 13:13:24,022[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-05 13:13:24,023[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-05 13:13:24,024[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-05 13:13:24,029[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-04T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-05 13:13:24,029[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:13:24,030[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-04T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-05 13:13:24,031[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:13:24,033[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:13:26,020[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-05 13:13:26,061[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-05 13:13:26,062[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:13:26,437[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:13:31,647[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-04T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-05 13:31:44,689[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:31:45,968[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-05 13:31:46,019[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-05 13:31:46,019[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:31:46,270[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:31:51,347[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-04T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-05 13:32:11,442[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-04T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-05 13:32:11,448[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-04T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-05 13:32:11,483[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-04T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-05 05:31:34.184729+00:00, run_end_date=2022-08-05 05:31:44.439745+00:00, run_duration=10.255016, state=success, executor_state=success, try_number=1, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-05 05:13:24.025741+00:00, queued_by_job_id=1, pid=67212[0m
[[34m2022-08-05 13:32:11,485[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-04T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-05 05:32:01.388559+00:00, run_end_date=2022-08-05 05:32:11.063674+00:00, run_duration=9.675115, state=success, executor_state=success, try_number=1, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-05 05:13:24.025741+00:00, queued_by_job_id=1, pid=67230[0m
[[34m2022-08-05 13:32:11,497[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=52788) last sent a heartbeat 56.45 seconds ago! Restarting it[0m
[[34m2022-08-05 13:32:11,511[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 52788. PIDs of all processes in the group: [52788][0m
[[34m2022-08-05 13:32:11,513[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 52788[0m
[[34m2022-08-05 13:32:11,975[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=52788, status='terminated', exitcode=0, started='15:41:45') (52788) terminated with exit code 0[0m
[[34m2022-08-05 13:32:12,013[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 67239[0m
[[34m2022-08-05 13:32:12,043[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-05 13:32:12,170] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-05 13:32:12,758[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-05 13:32:12,759[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-05 13:32:12,759[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-05 13:32:12,760[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-04T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-05 13:32:12,763[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-04T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-05 13:32:12,764[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:32:12,764[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-04T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-05 13:32:12,765[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:32:12,766[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:32:14,383[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-05 13:32:14,427[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-05 13:32:14,427[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:32:14,689[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:32:19,782[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-04T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-05 13:32:50,263[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-04T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-05 13:32:51,621[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-05 13:32:51,676[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-05 13:32:51,677[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:32:51,949[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-05 13:32:57,055[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-04T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-05 13:33:17,681[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-04T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-05 13:33:17,684[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-04T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-05 13:33:17,711[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-04T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-05 05:32:29.870762+00:00, run_end_date=2022-08-05 05:32:45.235200+00:00, run_duration=15.364438, state=success, executor_state=success, try_number=1, max_tries=3, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-05 05:32:12.761333+00:00, queued_by_job_id=1, pid=67291[0m
[[34m2022-08-05 13:33:17,712[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-04T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-05 05:33:07.241180+00:00, run_end_date=2022-08-05 05:33:17.437095+00:00, run_duration=10.195915, state=success, executor_state=success, try_number=1, max_tries=1, job_id=58, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-05 05:32:12.761333+00:00, queued_by_job_id=1, pid=67317[0m
[[34m2022-08-05 13:33:17,726[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=67239) last sent a heartbeat 65.01 seconds ago! Restarting it[0m
[[34m2022-08-05 13:33:17,736[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 67239. PIDs of all processes in the group: [67239][0m
[[34m2022-08-05 13:33:17,737[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 67239[0m
[[34m2022-08-05 13:33:18,195[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=67239, status='terminated', exitcode=0, started='13:32:12') (67239) terminated with exit code 0[0m
[[34m2022-08-05 13:33:18,203[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 67322[0m
[[34m2022-08-05 13:33:18,227[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-05 13:33:18,262] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-05 13:33:18,592[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-04 04:00:12.650445+00:00: scheduled__2022-08-04T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-05 13:33:18,593[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-04 04:00:12.650445+00:00, run_id=scheduled__2022-08-04T04:00:12.650445+00:00, run_start_date=2022-08-05 05:13:23.980669+00:00, run_end_date=2022-08-05 05:33:18.593165+00:00, run_duration=1194.612496, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-04 04:00:12.650445+00:00, data_interval_end=2022-08-05 04:00:12.650445+00:00, dag_hash=fc606fce991e7fcf99264d794c970934[0m
[[34m2022-08-05 13:33:18,598[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-05T04:00:12.650445+00:00, run_after=2022-08-06T04:00:12.650445+00:00[0m
[[34m2022-08-05 13:33:29,580[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 13:38:29,844[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 13:43:30,082[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 13:48:30,251[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 13:53:30,494[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 13:58:30,625[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:03:30,809[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:08:30,947[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:13:31,193[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:18:31,367[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:23:31,530[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:28:31,746[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:33:31,886[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:38:31,973[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:43:32,120[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:48:32,359[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:53:32,592[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 14:58:32,870[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:03:33,130[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:08:33,392[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:13:33,654[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:44:14,528[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:49:14,688[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:54:14,852[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 15:59:14,954[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 16:04:15,111[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 16:09:15,206[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 21:37:08,739[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:11:09,540[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:16:09,683[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:21:09,849[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:26:10,000[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:31:10,252[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:36:10,396[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:41:10,539[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:46:10,687[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:51:11,030[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 22:56:11,249[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:01:11,397[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:06:11,538[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:11:11,695[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:16:11,837[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:21:12,084[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:26:12,234[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:31:12,454[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:36:12,534[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:41:12,694[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:46:12,861[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:51:13,009[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-05 23:56:13,155[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:01:13,302[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:06:13,449[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:11:13,646[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:16:13,801[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:21:14,059[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:26:14,229[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:31:14,515[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:36:14,540[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:41:14,695[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:46:14,945[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:51:15,213[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 00:56:15,379[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 01:01:15,532[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 01:06:15,696[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 01:11:15,862[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 01:16:16,044[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 01:50:02,393[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 02:35:39,721[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 03:06:10,541[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 04:35:04,020[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 07:36:12,256[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-06 14:35:22,291[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-06T04:00:12.650445+00:00, run_after=2022-08-07T04:00:12.650445+00:00[0m
[[34m2022-08-06 14:35:22,354[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-06 14:35:22,355[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-06 14:35:22,356[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-06 14:35:22,356[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-06 14:35:22,364[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-05T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-06 14:35:22,364[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 14:35:22,365[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-05T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-06 14:35:22,366[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 14:35:22,369[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 14:35:24,416[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-06 14:35:24,472[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-06 14:35:24,473[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 14:35:24,873[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 14:35:30,038[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-05T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-06 17:05:31,801[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 17:05:33,032[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-06 17:05:33,077[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-06 17:05:33,078[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 17:05:33,310[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 19:44:43,960[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-05T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-06 19:46:06,198[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-05T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-06 19:46:06,205[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-05T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-06 19:46:06,265[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-05T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-06 08:05:26.821860+00:00, run_end_date=2022-08-06 09:05:31.535539+00:00, run_duration=3604.713679, state=success, executor_state=success, try_number=1, max_tries=1, job_id=59, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-06 06:35:22.357779+00:00, queued_by_job_id=1, pid=91320[0m
[[34m2022-08-06 19:46:06,267[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-05T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-06 11:44:54.162372+00:00, run_end_date=2022-08-06 11:46:05.910581+00:00, run_duration=71.748209, state=success, executor_state=success, try_number=1, max_tries=1, job_id=60, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-06 06:35:22.357779+00:00, queued_by_job_id=1, pid=91385[0m
[[34m2022-08-06 19:46:06,281[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=67322) last sent a heartbeat 63.56 seconds ago! Restarting it[0m
[[34m2022-08-06 19:46:06,301[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 67322. PIDs of all processes in the group: [67322][0m
[[34m2022-08-06 19:46:06,302[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 67322[0m
[[34m2022-08-06 19:46:07,081[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=67322, status='terminated', exitcode=0, started='2022-08-05 13:33:18') (67322) terminated with exit code 0[0m
[[34m2022-08-06 19:46:07,094[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 91396[0m
[[34m2022-08-06 19:46:07,138[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-06 19:46:07,192] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-06 19:46:07,540[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-06 19:46:07,541[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-06 19:46:07,542[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-06 19:46:07,542[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-05T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-06 19:46:07,546[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-05T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-06 19:46:07,546[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 19:46:07,547[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-05T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-06 19:46:07,547[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 19:46:07,549[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 19:46:08,840[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-06 19:46:08,879[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-06 19:46:08,879[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 19:46:09,118[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 21:08:31,945[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-05T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-06 21:09:02,381[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-05T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-06 21:09:04,751[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-06 21:09:04,883[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-06 21:09:04,884[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 21:09:05,189[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-06 21:09:10,290[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-05T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-06 21:09:30,887[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-05T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-06 21:09:30,893[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-05T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-06 21:09:30,951[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-05T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-06 13:08:42.020174+00:00, run_end_date=2022-08-06 13:08:56.488834+00:00, run_duration=14.46866, state=success, executor_state=success, try_number=1, max_tries=3, job_id=61, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-06 11:46:07.543573+00:00, queued_by_job_id=1, pid=91427[0m
[[34m2022-08-06 21:09:30,953[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-05T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-06 13:09:20.361442+00:00, run_end_date=2022-08-06 13:09:30.571665+00:00, run_duration=10.210223, state=success, executor_state=success, try_number=1, max_tries=1, job_id=62, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-06 11:46:07.543573+00:00, queued_by_job_id=1, pid=91521[0m
[[34m2022-08-06 21:09:30,967[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=91396) last sent a heartbeat 66.81 seconds ago! Restarting it[0m
[[34m2022-08-06 21:09:30,978[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 91396. PIDs of all processes in the group: [91396][0m
[[34m2022-08-06 21:09:30,979[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 91396[0m
[[34m2022-08-06 21:09:31,522[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=91396, status='terminated', exitcode=0, started='19:46:07') (91396) terminated with exit code 0[0m
[[34m2022-08-06 21:09:31,530[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 91529[0m
[[34m2022-08-06 21:09:31,554[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-06 21:09:31,592] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-06 21:09:31,934[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-05 04:00:12.650445+00:00: scheduled__2022-08-05T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-06 21:09:31,934[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-05 04:00:12.650445+00:00, run_id=scheduled__2022-08-05T04:00:12.650445+00:00, run_start_date=2022-08-06 06:35:22.306320+00:00, run_end_date=2022-08-06 13:09:31.934906+00:00, run_duration=23649.628586, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-05 04:00:12.650445+00:00, data_interval_end=2022-08-06 04:00:12.650445+00:00, dag_hash=36fe2e48276d7edc7e8c30baaec60692[0m
[[34m2022-08-06 21:09:31,940[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-06T04:00:12.650445+00:00, run_after=2022-08-07T04:00:12.650445+00:00[0m
[[34m2022-08-06 21:10:06,109[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 07:39:31,954[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 07:44:31,584[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 07:49:31,718[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 07:54:31,852[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 07:59:32,204[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 08:04:32,462[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 08:09:32,606[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 08:14:32,639[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 08:19:32,695[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 08:24:32,940[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 08:29:33,097[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 09:13:30,387[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 09:13:30,392[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-08-07 09:34:36,349[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 09:39:36,603[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 10:01:31,038[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 10:32:05,710[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 11:01:30,370[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 14:20:40,250[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-07T04:00:12.650445+00:00, run_after=2022-08-08T04:00:12.650445+00:00[0m
[[34m2022-08-07 14:20:40,311[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-07 14:20:40,367[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-07 14:20:40,375[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-07 14:20:40,375[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-07 14:20:40,389[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-06T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-07 14:20:40,390[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 14:20:40,391[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-06T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-07 14:20:40,391[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 14:20:40,394[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 14:20:42,606[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-07 14:20:42,656[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-07 14:20:42,657[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 14:20:43,184[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 14:20:48,389[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-06T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-07 18:20:54,811[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 18:20:56,094[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-07 18:20:56,139[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-07 18:20:56,140[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 18:20:56,409[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 19:20:54,670[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-06T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-07 23:06:35,950[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-06T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-07 23:06:35,956[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-06T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-07 23:06:35,994[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-06T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-07 07:20:49.889157+00:00, run_end_date=2022-08-07 10:20:54.496256+00:00, run_duration=10804.607099, state=success, executor_state=success, try_number=1, max_tries=1, job_id=63, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-07 06:20:40.379971+00:00, queued_by_job_id=1, pid=97830[0m
[[34m2022-08-07 23:06:35,995[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-06T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-07 12:45:05.146308+00:00, run_end_date=2022-08-07 15:06:30.724710+00:00, run_duration=8485.578402, state=success, executor_state=success, try_number=1, max_tries=1, job_id=64, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-07 06:20:40.379971+00:00, queued_by_job_id=1, pid=97873[0m
[[34m2022-08-07 23:06:36,009[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=91529) last sent a heartbeat 67.98 seconds ago! Restarting it[0m
[[34m2022-08-07 23:06:36,035[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 91529. PIDs of all processes in the group: [91529][0m
[[34m2022-08-07 23:06:36,038[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 91529[0m
[[34m2022-08-07 23:15:01,297[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=91529, status='terminated', exitcode=0, started='2022-08-06 21:09:31') (91529) terminated with exit code 0[0m
[[34m2022-08-07 23:15:01,311[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 97907[0m
[[34m2022-08-07 23:15:01,350[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-07 23:15:01,445] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-07 23:15:01,906[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-07 23:15:01,907[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-07 23:15:01,907[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-07 23:15:01,908[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-06T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-07 23:15:01,911[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-06T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-07 23:15:01,912[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 23:15:01,912[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-06T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-07 23:15:01,913[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 23:15:01,914[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 23:15:03,465[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-07 23:15:03,527[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-07 23:15:03,528[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 23:15:03,868[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 23:51:02,135[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-06T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-07 23:53:58,287[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-06T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-07 23:53:59,151[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-07 23:53:59,224[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-07 23:53:59,225[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 23:53:59,558[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-07 23:54:04,704[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-06T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-07 23:54:25,142[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-06T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-07 23:54:25,145[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-06T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-07 23:54:25,178[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-06T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-07 15:53:37.447482+00:00, run_end_date=2022-08-07 15:53:53.551659+00:00, run_duration=16.104177, state=success, executor_state=success, try_number=1, max_tries=3, job_id=65, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-07 15:15:01.909250+00:00, queued_by_job_id=1, pid=97952[0m
[[34m2022-08-07 23:54:25,179[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-06T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-07 15:54:14.754490+00:00, run_end_date=2022-08-07 15:54:24.921744+00:00, run_duration=10.167254, state=success, executor_state=success, try_number=1, max_tries=1, job_id=66, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-07 15:15:01.909250+00:00, queued_by_job_id=1, pid=98011[0m
[[34m2022-08-07 23:54:25,191[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=97907) last sent a heartbeat 74.82 seconds ago! Restarting it[0m
[[34m2022-08-07 23:54:25,202[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 97907. PIDs of all processes in the group: [97907][0m
[[34m2022-08-07 23:54:25,203[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 97907[0m
[[34m2022-08-07 23:54:25,623[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=97907, status='terminated', exitcode=0, started='23:15:01') (97907) terminated with exit code 0[0m
[[34m2022-08-07 23:54:25,633[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 98017[0m
[[34m2022-08-07 23:54:25,658[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-08-07 23:54:25,673[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-07 23:54:25,677[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2022-08-07 23:54:25,711] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-07 23:54:26,059[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-06 04:00:12.650445+00:00: scheduled__2022-08-06T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-07 23:54:26,060[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-06 04:00:12.650445+00:00, run_id=scheduled__2022-08-06T04:00:12.650445+00:00, run_start_date=2022-08-07 06:20:40.263646+00:00, run_end_date=2022-08-07 15:54:26.060457+00:00, run_duration=34425.796811, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-06 04:00:12.650445+00:00, data_interval_end=2022-08-07 04:00:12.650445+00:00, dag_hash=88b6753e92d1823eb678665fcc4566c9[0m
[[34m2022-08-07 23:54:26,065[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-07T04:00:12.650445+00:00, run_after=2022-08-08T04:00:12.650445+00:00[0m
[[34m2022-08-07 23:59:25,830[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:04:26,074[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:09:26,505[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:14:26,747[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:19:26,799[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:35:54,337[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:40:54,360[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:45:54,486[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:50:54,611[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 00:55:54,929[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 01:00:55,062[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 01:39:02,090[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 02:38:51,259[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 03:09:44,278[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 07:10:22,187[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 12:19:55,877[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-08T04:00:12.650445+00:00, run_after=2022-08-09T04:00:12.650445+00:00[0m
[[34m2022-08-08 12:19:55,921[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-08 12:19:55,921[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-08 12:19:55,922[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-08 12:19:55,922[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-08 12:19:55,926[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-07T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-08 12:19:55,927[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 12:19:55,928[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-07T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-08 12:19:55,928[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 12:19:55,930[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 12:19:58,169[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-08 12:19:58,280[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-08 12:19:58,280[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-08 12:19:58,691[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-08 12:20:03,869[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-07T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-08 16:20:10,243[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 16:20:11,452[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-08 16:20:11,499[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-08 16:20:11,499[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-08 16:20:11,729[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-08 17:20:09,411[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-07T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-08 20:45:11,810[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-07T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-08 20:45:11,816[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-07T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-08 20:45:11,866[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-07T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-08 05:20:05.191004+00:00, run_end_date=2022-08-08 08:20:09.954170+00:00, run_duration=10804.763166, state=success, executor_state=success, try_number=1, max_tries=1, job_id=67, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-08 04:19:55.923617+00:00, queued_by_job_id=1, pid=18781[0m
[[34m2022-08-08 20:45:11,868[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-07T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-08 11:22:35.647611+00:00, run_end_date=2022-08-08 12:45:07.105730+00:00, run_duration=4951.458119, state=success, executor_state=success, try_number=1, max_tries=1, job_id=68, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-08 04:19:55.923617+00:00, queued_by_job_id=1, pid=18830[0m
[[34m2022-08-08 20:45:11,882[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=98017) last sent a heartbeat 70.76 seconds ago! Restarting it[0m
[[34m2022-08-08 20:45:11,902[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 98017. PIDs of all processes in the group: [98017][0m
[[34m2022-08-08 20:45:11,904[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 98017[0m
[[34m2022-08-08 20:45:12,447[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=98017, status='terminated', exitcode=0, started='23:54:25') (98017) terminated with exit code 0[0m
[[34m2022-08-08 20:45:12,461[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 18854[0m
[[34m2022-08-08 20:45:12,488[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-08-08 20:45:12,500[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-08 20:45:12,506[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2022-08-08 20:45:12,534] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-08 20:45:12,778[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-08 20:45:12,779[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-08 20:45:12,779[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-08 20:45:12,780[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-07T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-08 20:45:12,783[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-07T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-08 20:45:12,784[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 20:45:12,784[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-07T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-08 20:45:12,785[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 20:45:12,786[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-08 20:45:14,004[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-08 20:45:14,036[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-08 20:45:14,036[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-08 20:45:14,272[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-08 21:50:14,245[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-07T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 00:00:07,325[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 00:00:08,204[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-09 00:00:08,234[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-09 00:00:08,235[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 03:20:16,915[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 03:20:22,305[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-07T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 08:50:27,812[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-07T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-08-09 08:50:27,823[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-07T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-09 08:50:27,825[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-07T04:00:12.650445+00:00 exited with status failed for try_number 1[0m
[[34m2022-08-09 08:50:27,870[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-07T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-08 14:50:13.665233+00:00, run_end_date=2022-08-08 16:00:07.144937+00:00, run_duration=4193.479704, state=success, executor_state=success, try_number=1, max_tries=3, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-08 12:45:12.781258+00:00, queued_by_job_id=1, pid=18892[0m
[[34m2022-08-09 08:50:27,871[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-07T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-08 20:20:21.855097+00:00, run_end_date=2022-08-08 20:20:32.427349+00:00, run_duration=10.572252, state=success, executor_state=failed, try_number=1, max_tries=1, job_id=70, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-08 12:45:12.781258+00:00, queued_by_job_id=1, pid=18946[0m
[[34m2022-08-09 08:50:27,885[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=18854) last sent a heartbeat 74.30 seconds ago! Restarting it[0m
[[34m2022-08-09 08:50:27,905[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 18854. PIDs of all processes in the group: [18854][0m
[[34m2022-08-09 08:50:27,907[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 18854[0m
[[34m2022-08-09 08:50:28,697[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=18854, status='terminated', exitcode=0, started='20:45:12') (18854) terminated with exit code 0[0m
[[34m2022-08-09 08:50:28,706[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 18974[0m
[[34m2022-08-09 08:50:28,723[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-09 08:50:28,756] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-09 08:50:29,096[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-07 04:00:12.650445+00:00: scheduled__2022-08-07T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-09 08:50:29,097[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-07 04:00:12.650445+00:00, run_id=scheduled__2022-08-07T04:00:12.650445+00:00, run_start_date=2022-08-08 04:19:55.891066+00:00, run_end_date=2022-08-09 00:50:29.097446+00:00, run_duration=73833.20638, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-07 04:00:12.650445+00:00, data_interval_end=2022-08-08 04:00:12.650445+00:00, dag_hash=130606686d33d81eccb0ab3d52aee796[0m
[[34m2022-08-09 08:50:29,102[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-08T04:00:12.650445+00:00, run_after=2022-08-09T04:00:12.650445+00:00[0m
[[34m2022-08-09 12:00:14,084[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-09T04:00:12.650445+00:00, run_after=2022-08-10T04:00:12.650445+00:00[0m
[[34m2022-08-09 12:00:14,121[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 12:00:14,122[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-09 12:00:14,122[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-09 12:00:14,123[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 12:00:14,126[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-08T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-09 12:00:14,127[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 12:00:14,127[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-08T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-09 12:00:14,128[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 12:00:14,130[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 18:00:06,211[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-09 18:00:06,386[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-09 18:00:06,387[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 18:00:06,904[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 18:00:12,094[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 20:45:07,984[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py']' returned non-zero exit status 1..[0m
[[34m2022-08-09 20:45:07,996[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 20:45:09,842[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-09 20:45:09,938[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-09 20:45:09,939[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 20:45:10,312[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 20:45:15,517[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-08T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 21:11:27,338[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-08T04:00:12.650445+00:00 exited with status failed for try_number 1[0m
[[34m2022-08-09 21:11:27,339[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-08T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-09 21:11:27,368[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-08T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-09 11:00:10.774293+00:00, run_end_date=None, run_duration=None, state=running, executor_state=failed, try_number=1, max_tries=1, job_id=71, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-09 04:00:14.123959+00:00, queued_by_job_id=1, pid=None[0m
[[34m2022-08-09 21:11:27,368[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-08T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-09 13:11:16.949786+00:00, run_end_date=2022-08-09 13:11:27.110143+00:00, run_duration=10.160357, state=success, executor_state=success, try_number=1, max_tries=1, job_id=72, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-09 04:00:14.123959+00:00, queued_by_job_id=1, pid=19171[0m
[[34m2022-08-09 21:11:27,381[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=18974) last sent a heartbeat 64.41 seconds ago! Restarting it[0m
[[34m2022-08-09 21:11:27,392[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 18974. PIDs of all processes in the group: [18974][0m
[[34m2022-08-09 21:11:27,393[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 18974[0m
[[34m2022-08-09 21:11:27,730[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=18974, status='terminated', exitcode=0, started='08:50:28') (18974) terminated with exit code 0[0m
[[34m2022-08-09 21:11:27,738[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 19177[0m
[[34m2022-08-09 21:11:27,753[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-08-09 21:11:27,767[0m] {[34mscheduler_job.py:[0m1362} WARNING[0m - Failing (1) jobs without heartbeat after 2022-08-09 13:06:27.760796+00:00[0m
[[34m2022-08-09 21:11:27,768[0m] {[34mscheduler_job.py:[0m1370} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/pro/Documents/proj_data/airflow/dags/test.py', 'msg': 'Detected <TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [running]> as zombie', 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff865130c90>, 'is_failure_callback': True}[0m
[2022-08-09 21:11:27,778] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-09 21:11:28,091[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 21:11:28,092[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-09 21:11:28,093[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 21:11:28,102[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-08T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-09 21:11:28,102[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 21:11:28,104[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 21:11:29,341[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-09 21:11:29,380[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-09 21:11:29,380[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 21:11:29,648[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 21:11:32,802[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-08T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 21:11:53,589[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-08T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-09 21:11:53,645[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-08T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-09 13:11:42.912929+00:00, run_end_date=2022-08-09 13:11:53.156921+00:00, run_duration=10.243992, state=success, executor_state=success, try_number=1, max_tries=1, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-09 13:11:28.094260+00:00, queued_by_job_id=1, pid=19226[0m
[[34m2022-08-09 21:13:23,220[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:16:28,539[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 21:16:28,541[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-09 21:16:28,541[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 21:16:28,545[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-08T04:00:12.650445+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-09 21:16:28,545[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 21:16:28,547[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 21:16:29,942[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-09 21:16:29,978[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-09 21:16:29,978[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 21:16:30,251[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 21:16:35,338[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-08T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 21:16:55,757[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-08T04:00:12.650445+00:00 exited with status success for try_number 2[0m
[[34m2022-08-09 21:16:55,766[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-08T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-09 13:16:45.380576+00:00, run_end_date=2022-08-09 13:16:55.558158+00:00, run_duration=10.177582, state=success, executor_state=success, try_number=2, max_tries=1, job_id=74, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-09 13:16:28.542539+00:00, queued_by_job_id=1, pid=19629[0m
[[34m2022-08-09 21:16:55,940[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 21:16:55,940[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-09 21:16:55,941[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-08T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-09 21:16:55,943[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-08T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-09 21:16:55,944[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 21:16:55,946[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-08T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-09 21:16:56,909[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-09 21:16:56,942[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-09 21:16:56,943[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 21:16:57,136[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-09 21:17:02,206[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-08T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-09 21:17:32,562[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-08T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-09 21:17:32,570[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-08T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-09 13:17:12.252171+00:00, run_end_date=2022-08-09 13:17:27.442838+00:00, run_duration=15.190667, state=success, executor_state=success, try_number=1, max_tries=3, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-09 13:16:55.942132+00:00, queued_by_job_id=1, pid=19636[0m
[[34m2022-08-09 21:17:32,731[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-08 04:00:12.650445+00:00: scheduled__2022-08-08T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-09 21:17:32,732[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-08 04:00:12.650445+00:00, run_id=scheduled__2022-08-08T04:00:12.650445+00:00, run_start_date=2022-08-09 04:00:14.094809+00:00, run_end_date=2022-08-09 13:17:32.732118+00:00, run_duration=33438.637309, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-08 04:00:12.650445+00:00, data_interval_end=2022-08-09 04:00:12.650445+00:00, dag_hash=130606686d33d81eccb0ab3d52aee796[0m
[[34m2022-08-09 21:17:32,737[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-09T04:00:12.650445+00:00, run_after=2022-08-10T04:00:12.650445+00:00[0m
[[34m2022-08-09 21:18:23,373[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:23:23,458[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:28:24,175[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:33:24,323[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:38:24,469[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:43:24,620[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:48:24,776[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:53:25,033[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 21:58:25,187[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:03:25,444[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:08:25,598[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:13:25,772[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:19:54,875[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:24:55,041[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:29:55,195[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:34:55,360[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:52:01,289[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 22:57:01,447[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:02:01,595[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:07:01,842[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:12:01,984[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:17:02,133[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:22:02,220[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:27:02,408[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:32:02,556[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:37:02,708[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:42:02,857[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:47:03,042[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:52:03,315[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-09 23:57:03,574[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:02:03,748[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:07:03,894[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:12:04,052[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:17:04,075[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:22:04,172[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:27:04,380[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:32:04,563[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:37:04,725[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:42:04,980[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:47:05,138[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:52:05,322[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 00:57:05,479[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 01:02:05,626[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 01:07:05,888[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 01:12:06,154[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 01:17:06,421[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 01:22:06,682[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 01:27:06,811[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 03:05:25,813[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 03:05:25,817[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-08-10 13:31:40,247[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-10T04:00:12.650445+00:00, run_after=2022-08-11T04:00:12.650445+00:00[0m
[[34m2022-08-10 13:31:40,314[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-10 13:31:40,315[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-10 13:31:40,316[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-10 13:31:40,316[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-10 13:31:40,344[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='scheduled__2022-08-09T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-10 13:31:40,347[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 13:31:40,349[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='scheduled__2022-08-09T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-08-10 13:31:40,350[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 13:31:40,354[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 13:31:42,872[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-10 13:31:42,906[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-10 13:31:42,906[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 13:31:43,282[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 13:31:48,388[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date scheduled__2022-08-09T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-10 19:00:12,249[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 19:00:13,486[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-10 19:00:13,538[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-10 19:00:13,539[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 19:00:13,765[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 19:46:03,623[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello scheduled__2022-08-09T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-10 20:46:12,345[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=scheduled__2022-08-09T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-10 20:46:12,348[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=scheduled__2022-08-09T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-10 20:46:12,375[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=scheduled__2022-08-09T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-10 07:16:09.042838+00:00, run_end_date=2022-08-10 11:00:11.983020+00:00, run_duration=13442.940182, state=success, executor_state=success, try_number=1, max_tries=1, job_id=76, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-08-10 05:31:40.317483+00:00, queued_by_job_id=1, pid=38286[0m
[[34m2022-08-10 20:46:12,376[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=scheduled__2022-08-09T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-10 11:46:13.766645+00:00, run_end_date=2022-08-10 12:46:12.132192+00:00, run_duration=3598.365547, state=success, executor_state=success, try_number=1, max_tries=1, job_id=77, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-08-10 05:31:40.317483+00:00, queued_by_job_id=1, pid=38324[0m
[[34m2022-08-10 20:46:12,389[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=19177) last sent a heartbeat 61.21 seconds ago! Restarting it[0m
[[34m2022-08-10 20:46:12,406[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 19177. PIDs of all processes in the group: [19177][0m
[[34m2022-08-10 20:46:12,409[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 19177[0m
[[34m2022-08-10 21:36:20,792[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=19177, status='terminated', exitcode=0, started='2022-08-09 21:11:27') (19177) terminated with exit code 0[0m
[[34m2022-08-10 21:36:20,807[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 38353[0m
[[34m2022-08-10 21:36:20,835[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-10 21:36:20,887] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-10 21:36:21,978[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-10 21:36:21,979[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-08-10 21:36:21,979[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-08-10 21:36:21,986[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye scheduled__2022-08-09T04:00:12.650445+00:00 [scheduled]>[0m
[[34m2022-08-10 21:36:21,989[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='scheduled__2022-08-09T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-10 21:36:21,990[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 21:36:21,991[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='scheduled__2022-08-09T04:00:12.650445+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-08-10 21:36:21,992[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 21:36:21,995[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 21:36:25,722[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-10 21:36:25,760[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-10 21:36:25,760[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 21:36:26,285[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 21:36:31,501[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep scheduled__2022-08-09T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-10 21:37:01,497[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'scheduled__2022-08-09T04:00:12.650445+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-08-10 21:37:02,819[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-08-10 21:37:02,881[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-08-10 21:37:02,882[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 21:37:03,132[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-08-10 21:37:08,233[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye scheduled__2022-08-09T04:00:12.650445+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-08-10 21:37:28,725[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=scheduled__2022-08-09T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-10 21:37:28,731[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=scheduled__2022-08-09T04:00:12.650445+00:00 exited with status success for try_number 1[0m
[[34m2022-08-10 21:37:28,783[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=scheduled__2022-08-09T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-10 13:36:40.865997+00:00, run_end_date=2022-08-10 13:36:56.334558+00:00, run_duration=15.468561, state=success, executor_state=success, try_number=1, max_tries=3, job_id=78, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-08-10 13:36:21.987278+00:00, queued_by_job_id=1, pid=38384[0m
[[34m2022-08-10 21:37:28,783[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=scheduled__2022-08-09T04:00:12.650445+00:00, map_index=-1, run_start_date=2022-08-10 13:37:18.294708+00:00, run_end_date=2022-08-10 13:37:28.463523+00:00, run_duration=10.168815, state=success, executor_state=success, try_number=1, max_tries=1, job_id=79, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-08-10 13:36:21.987278+00:00, queued_by_job_id=1, pid=38468[0m
[[34m2022-08-10 21:37:28,798[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=38353) last sent a heartbeat 67.59 seconds ago! Restarting it[0m
[[34m2022-08-10 21:37:28,811[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 38353. PIDs of all processes in the group: [38353][0m
[[34m2022-08-10 21:37:28,812[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 38353[0m
[[34m2022-08-10 21:37:29,393[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=38353, status='terminated', exitcode=0, started='21:36:20') (38353) terminated with exit code 0[0m
[[34m2022-08-10 21:37:29,403[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 38483[0m
[[34m2022-08-10 21:37:29,429[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-08-10 21:37:29,470] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-08-10 21:37:29,821[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-08-09 04:00:12.650445+00:00: scheduled__2022-08-09T04:00:12.650445+00:00, externally triggered: False> successful[0m
[[34m2022-08-10 21:37:29,823[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-08-09 04:00:12.650445+00:00, run_id=scheduled__2022-08-09T04:00:12.650445+00:00, run_start_date=2022-08-10 05:31:40.262560+00:00, run_end_date=2022-08-10 13:37:29.823007+00:00, run_duration=29149.560447, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-09 04:00:12.650445+00:00, data_interval_end=2022-08-10 04:00:12.650445+00:00, dag_hash=e7f4961846357913149835f35551bcd2[0m
[[34m2022-08-10 21:37:29,828[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-08-10T04:00:12.650445+00:00, run_after=2022-08-11T04:00:12.650445+00:00[0m
[[34m2022-08-10 21:38:12,811[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 21:43:13,053[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 21:48:13,067[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 21:53:13,170[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 21:58:13,303[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:03:13,434[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:08:13,507[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:13:13,659[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:18:13,805[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:23:14,049[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:28:14,300[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:33:14,452[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:38:14,529[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:47:57,025[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:52:57,379[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 22:57:57,674[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:02:57,705[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:07:57,856[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:12:58,023[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:17:58,205[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:22:58,553[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:27:58,905[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:32:59,068[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:37:59,297[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:42:59,582[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:47:59,753[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:52:59,917[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-10 23:58:00,180[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 00:03:00,350[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 00:08:00,501[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 00:13:00,660[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 00:18:00,809[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 00:23:00,964[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 00:28:01,243[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 01:31:52,819[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 01:31:52,824[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-08-11 07:46:25,090[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 09:46:38,801[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 09:51:38,860[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 09:56:39,013[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 10:01:39,170[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-08-11 10:24:22,843[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
