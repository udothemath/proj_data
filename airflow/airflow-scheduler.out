[[34m2022-07-24 23:12:02,584[0m] {[34mscheduler_job.py:[0m708} INFO[0m - Starting the scheduler[0m
[[34m2022-07-24 23:12:02,585[0m] {[34mscheduler_job.py:[0m713} INFO[0m - Processing each file at most -1 times[0m
[[34m2022-07-24 23:12:02,590[0m] {[34mexecutor_loader.py:[0m105} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2022-07-24 23:12:02,594[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 12388[0m
[[34m2022-07-24 23:12:02,596[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:12:02,602[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-24 23:12:02,617] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-24 23:17:02,690[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:22:02,928[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:27:03,068[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:32:03,220[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:37:03,372[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:42:03,624[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-24 23:51:38,096[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 00:58:03,137[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 01:30:33,507[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 08:57:53,493[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 08:57:53,505[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-25 21:48:31,978[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 21:53:32,118[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 21:58:32,253[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 22:03:32,735[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 22:56:36,943[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 22:56:36,948[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-07-25 23:01:36,899[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:06:37,137[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:11:37,382[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:12:51,131[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:12:45.850485+00:00, run_after=2022-07-26T15:12:45.850485+00:00[0m
[[34m2022-07-25 23:12:51,175[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-24 15:12:45.850485+00:00: scheduled__2022-07-24T15:12:45.850485+00:00, externally triggered: False> successful[0m
[[34m2022-07-25 23:12:51,180[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-24 15:12:45.850485+00:00, run_id=scheduled__2022-07-24T15:12:45.850485+00:00, run_start_date=2022-07-25 15:12:51.144059+00:00, run_end_date=2022-07-25 15:12:51.180860+00:00, run_duration=0.036801, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-07-24 15:12:45.850485+00:00, data_interval_end=2022-07-25 15:12:45.850485+00:00, dag_hash=c9827c605aef0d340979e0977662107b[0m
[[34m2022-07-25 23:12:51,184[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:12:45.850485+00:00, run_after=2022-07-26T15:12:45.850485+00:00[0m
[[34m2022-07-25 23:16:37,569[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:21:37,745[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:26:37,949[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:27:50,647[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:27:50,650[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:27:50,651[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:27:50,663[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T15:27:49.983292+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-25 23:27:50,664[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:27:50,667[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:27:52,182[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:27:52,229[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:27:52,229[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:27:52,533[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:27:57,605[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T15:27:49.983292+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:28:18,040[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T15:27:49.983292+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:28:18,057[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T15:27:49.983292+00:00, map_index=-1, run_start_date=2022-07-25 15:28:07.645489+00:00, run_end_date=2022-07-25 15:28:17.798813+00:00, run_duration=10.153324, state=success, executor_state=success, try_number=1, max_tries=1, job_id=2, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 15:27:50.652908+00:00, queued_by_job_id=1, pid=23792[0m
[[34m2022-07-25 23:28:18,233[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:28:18,234[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:28:18,235[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:27:49.983292+00:00 [scheduled]>[0m
[[34m2022-07-25 23:28:18,238[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T15:27:49.983292+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-25 23:28:18,239[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:28:18,240[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:27:49.983292+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:28:19,419[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:28:19,452[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:28:19,453[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:28:19,684[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:28:24,757[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T15:27:49.983292+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:28:45,126[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T15:27:49.983292+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:28:45,136[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T15:27:49.983292+00:00, map_index=-1, run_start_date=2022-07-25 15:28:34.800605+00:00, run_end_date=2022-07-25 15:28:44.944355+00:00, run_duration=10.14375, state=success, executor_state=success, try_number=1, max_tries=1, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 15:28:18.237024+00:00, queued_by_job_id=1, pid=23801[0m
[[34m2022-07-25 23:28:45,288[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 15:27:49.983292+00:00: manual__2022-07-25T15:27:49.983292+00:00, externally triggered: True> successful[0m
[[34m2022-07-25 23:28:45,289[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 15:27:49.983292+00:00, run_id=manual__2022-07-25T15:27:49.983292+00:00, run_start_date=2022-07-25 15:27:50.591179+00:00, run_end_date=2022-07-25 15:28:45.289513+00:00, run_duration=54.698334, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 15:27:49.983292+00:00, data_interval_end=2022-07-25 15:27:49.983292+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-25 23:28:45,292[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:27:49.983292+00:00, run_after=2022-07-26T15:27:49.983292+00:00[0m
[[34m2022-07-25 23:31:38,248[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:36:38,435[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:41:38,653[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:46:38,842[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:51:39,019[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-25 23:52:13,854[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:13,856[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:52:13,857[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:13,860[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T15:52:12.586954+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-25 23:52:13,860[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:13,862[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:15,080[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:52:15,114[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:52:15,115[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:15,361[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:20,436[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T15:52:12.586954+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:52:40,861[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T15:52:12.586954+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:52:40,871[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T15:52:12.586954+00:00, map_index=-1, run_start_date=2022-07-25 15:52:30.481164+00:00, run_end_date=2022-07-25 15:52:40.632446+00:00, run_duration=10.151282, state=success, executor_state=success, try_number=1, max_tries=1, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 15:52:13.857980+00:00, queued_by_job_id=1, pid=26902[0m
[[34m2022-07-25 23:52:41,044[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:41,045[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-25 23:52:41,045[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T15:52:12.586954+00:00 [scheduled]>[0m
[[34m2022-07-25 23:52:41,048[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T15:52:12.586954+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-25 23:52:41,048[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:41,050[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T15:52:12.586954+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-25 23:52:42,270[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-25 23:52:42,308[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-25 23:52:42,309[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:42,548[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-25 23:52:47,625[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T15:52:12.586954+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-25 23:53:08,035[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T15:52:12.586954+00:00 exited with status success for try_number 1[0m
[[34m2022-07-25 23:53:08,044[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T15:52:12.586954+00:00, map_index=-1, run_start_date=2022-07-25 15:52:57.669107+00:00, run_end_date=2022-07-25 15:53:07.821243+00:00, run_duration=10.152136, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 15:52:41.046721+00:00, queued_by_job_id=1, pid=26914[0m
[[34m2022-07-25 23:53:08,192[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 15:52:12.586954+00:00: manual__2022-07-25T15:52:12.586954+00:00, externally triggered: True> successful[0m
[[34m2022-07-25 23:53:08,193[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 15:52:12.586954+00:00, run_id=manual__2022-07-25T15:52:12.586954+00:00, run_start_date=2022-07-25 15:52:13.831843+00:00, run_end_date=2022-07-25 15:53:08.193561+00:00, run_duration=54.361718, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 15:52:12.586954+00:00, data_interval_end=2022-07-25 15:52:12.586954+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-25 23:53:08,196[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T15:52:12.586954+00:00, run_after=2022-07-26T15:52:12.586954+00:00[0m
[[34m2022-07-25 23:56:39,219[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 00:00:39,764[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:00:39,766[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:00:39,767[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:00:39,770[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T16:00:01+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 00:00:39,770[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:00:39,772[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:00:41,089[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:00:41,135[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:00:41,135[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:00:41,382[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:00:46,475[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:01+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:01:06,873[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T16:00:01+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:01:06,882[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T16:00:01+00:00, map_index=-1, run_start_date=2022-07-25 16:00:56.516906+00:00, run_end_date=2022-07-25 16:01:06.678815+00:00, run_duration=10.161909, state=success, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 16:00:39.768037+00:00, queued_by_job_id=1, pid=27608[0m
[[34m2022-07-26 00:01:07,063[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:01:07,064[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:01:07,064[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 00:01:07,065[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:01+00:00 [scheduled]>[0m
[[34m2022-07-26 00:01:07,068[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T16:00:58.056951+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 00:01:07,068[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:07,069[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T16:00:01+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 00:01:07,069[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:07,071[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:08,271[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:01:08,323[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:01:08,323[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:08,560[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:13,645[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T16:00:58.056951+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:01:34,041[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:01+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:01:35,369[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:01:35,417[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:01:35,417[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:35,648[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:01:40,739[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:01+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:02:01,122[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T16:00:58.056951+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:02:01,124[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T16:00:01+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:02:01,139[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T16:00:01+00:00, map_index=-1, run_start_date=2022-07-25 16:01:50.781240+00:00, run_end_date=2022-07-25 16:02:00.936189+00:00, run_duration=10.154949, state=success, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 16:01:07.066432+00:00, queued_by_job_id=1, pid=27682[0m
[[34m2022-07-26 00:02:01,140[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T16:00:58.056951+00:00, map_index=-1, run_start_date=2022-07-25 16:01:23.684437+00:00, run_end_date=2022-07-25 16:01:33.860610+00:00, run_duration=10.176173, state=success, executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 16:01:07.066432+00:00, queued_by_job_id=1, pid=27616[0m
[[34m2022-07-26 00:02:01,151[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=12388) last sent a heartbeat 54.12 seconds ago! Restarting it[0m
[[34m2022-07-26 00:02:01,159[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 12388. PIDs of all processes in the group: [12388][0m
[[34m2022-07-26 00:02:01,160[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 12388[0m
[[34m2022-07-26 00:02:01,739[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=12388, status='terminated', exitcode=0, started='2022-07-24 23:12:02') (12388) terminated with exit code 0[0m
[[34m2022-07-26 00:02:01,746[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 27691[0m
[[34m2022-07-26 00:02:01,760[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-07-26 00:02:01,777[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 00:02:01,780[0m] {[34mscheduler_job.py:[0m1256} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2022-07-26 00:02:01,783] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-26 00:02:01,997[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 16:00:01+00:00: manual__2022-07-25T16:00:01+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 00:02:01,997[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 16:00:01+00:00, run_id=manual__2022-07-25T16:00:01+00:00, run_start_date=2022-07-25 16:00:39.741362+00:00, run_end_date=2022-07-25 16:02:01.997947+00:00, run_duration=82.256585, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 16:00:01+00:00, data_interval_end=2022-07-25 16:00:01+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-26 00:02:02,001[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T16:00:01+00:00, run_after=2022-07-26T16:00:01+00:00[0m
[[34m2022-07-26 00:02:02,013[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>[0m
[[34m2022-07-26 00:02:02,013[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:02:02,014[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:58.056951+00:00 [scheduled]>[0m
[[34m2022-07-26 00:02:02,016[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T16:00:58.056951+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 00:02:02,017[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:02:02,018[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:00:58.056951+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:02:03,253[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:02:03,290[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:02:03,290[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:02:03,541[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:02:08,655[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T16:00:58.056951+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:02:29,035[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T16:00:58.056951+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:02:29,045[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T16:00:58.056951+00:00, map_index=-1, run_start_date=2022-07-25 16:02:18.697098+00:00, run_end_date=2022-07-25 16:02:28.854638+00:00, run_duration=10.15754, state=success, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 16:02:02.015070+00:00, queued_by_job_id=1, pid=27709[0m
[[34m2022-07-26 00:02:29,193[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 16:00:58.056951+00:00: manual__2022-07-25T16:00:58.056951+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 00:02:29,194[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 16:00:58.056951+00:00, run_id=manual__2022-07-25T16:00:58.056951+00:00, run_start_date=2022-07-25 16:01:07.038383+00:00, run_end_date=2022-07-25 16:02:29.194174+00:00, run_duration=82.155791, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 16:00:58.056951+00:00, data_interval_end=2022-07-25 16:00:58.056951+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-26 00:02:29,196[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T16:00:58.056951+00:00, run_after=2022-07-26T16:00:58.056951+00:00[0m
[[34m2022-07-26 00:03:28,191[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:28,193[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:03:28,193[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:28,197[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T16:02:53+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 00:03:28,198[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:28,199[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:29,574[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:03:29,642[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:03:29,642[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:03:29,893[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:03:34,975[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T16:02:53+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:03:55,369[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T16:02:53+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:03:55,378[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T16:02:53+00:00, map_index=-1, run_start_date=2022-07-25 16:03:45.018125+00:00, run_end_date=2022-07-25 16:03:55.176607+00:00, run_duration=10.158482, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 16:03:28.194910+00:00, queued_by_job_id=1, pid=27837[0m
[[34m2022-07-26 00:03:55,547[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:55,547[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 00:03:55,548[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T16:02:53+00:00 [scheduled]>[0m
[[34m2022-07-26 00:03:55,550[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T16:02:53+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 00:03:55,551[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:55,552[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T16:02:53+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 00:03:56,464[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 00:03:56,493[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 00:03:56,494[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:03:56,695[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 00:04:01,773[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T16:02:53+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 00:04:22,161[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T16:02:53+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 00:04:22,172[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T16:02:53+00:00, map_index=-1, run_start_date=2022-07-25 16:04:11.816277+00:00, run_end_date=2022-07-25 16:04:21.967684+00:00, run_duration=10.151407, state=success, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 16:03:55.549349+00:00, queued_by_job_id=1, pid=27846[0m
[[34m2022-07-26 00:04:22,328[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 16:02:53+00:00: manual__2022-07-25T16:02:53+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 00:04:22,329[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 16:02:53+00:00, run_id=manual__2022-07-25T16:02:53+00:00, run_start_date=2022-07-25 16:03:28.170645+00:00, run_end_date=2022-07-25 16:04:22.329246+00:00, run_duration=54.158601, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 16:02:53+00:00, data_interval_end=2022-07-25 16:02:53+00:00, dag_hash=d2605ad2ff248909a594b62885f278b3[0m
[[34m2022-07-26 00:04:22,331[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T16:02:53+00:00, run_after=2022-07-26T16:02:53+00:00[0m
[[34m2022-07-26 00:07:11,364[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 00:59:22,404[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 01:59:34,468[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:02:20,285[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:07:20,428[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:12:20,743[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:17:20,888[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:22:21,032[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:27:21,200[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:32:21,300[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:34:42,463[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.print_date manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:34:42,464[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 04:34:42,465[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 04:34:42,465[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.print_date manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_hello manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:34:42,468[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='print_date', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 04:34:42,469[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:34:42,469[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_hello', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-07-26 04:34:42,470[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:34:42,472[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'print_date', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:34:43,761[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:34:43,816[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:34:43,817[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:34:44,079[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:34:49,162[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.print_date manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:35:09,630[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_hello', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:10,843[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:35:10,895[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:35:10,896[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:11,182[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:16,294[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_hello manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:35:36,681[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.print_date run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:35:36,683[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_hello run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:35:36,693[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=print_date, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:34:59.198130+00:00, run_end_date=2022-07-25 20:35:09.397227+00:00, run_duration=10.199097, state=success, executor_state=success, try_number=1, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-07-25 20:34:42.466541+00:00, queued_by_job_id=1, pid=32943[0m
[[34m2022-07-26 04:35:36,694[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_hello, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:35:26.340540+00:00, run_end_date=2022-07-25 20:35:36.505597+00:00, run_duration=10.165057, state=success, executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2022-07-25 20:34:42.466541+00:00, queued_by_job_id=1, pid=32948[0m
[[34m2022-07-26 04:35:36,705[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=27691) last sent a heartbeat 54.29 seconds ago! Restarting it[0m
[[34m2022-07-26 04:35:36,715[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 27691. PIDs of all processes in the group: [27691][0m
[[34m2022-07-26 04:35:36,717[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 27691[0m
[[34m2022-07-26 04:35:37,096[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=27691, status='terminated', exitcode=0, started='00:02:01') (27691) terminated with exit code 0[0m
[[34m2022-07-26 04:35:37,102[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 32949[0m
[[34m2022-07-26 04:35:37,115[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-26 04:35:37,142] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-26 04:35:37,360[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 2 tasks up for execution:
	<TaskInstance: a_test.sleep manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:35:37,361[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 0/16 running and queued tasks[0m
[[34m2022-07-26 04:35:37,361[0m] {[34mscheduler_job.py:[0m422} INFO[0m - DAG a_test has 1/16 running and queued tasks[0m
[[34m2022-07-26 04:35:37,362[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_test.sleep manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>
	<TaskInstance: a_test.test_say_bye manual__2022-07-25T20:34:41.637079+00:00 [scheduled]>[0m
[[34m2022-07-26 04:35:37,364[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='sleep', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 04:35:37,365[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:37,366[0m] {[34mscheduler_job.py:[0m546} INFO[0m - Sending TaskInstanceKey(dag_id='a_test', task_id='test_say_bye', run_id='manual__2022-07-25T20:34:41.637079+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-07-26 04:35:37,366[0m] {[34mbase_executor.py:[0m91} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:37,368[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'sleep', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:35:38,558[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:35:38,597[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:35:38,597[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:38,830[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:35:43,914[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.sleep manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:36:14,242[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_test', 'test_say_bye', 'manual__2022-07-25T20:34:41.637079+00:00', '--local', '--subdir', 'DAGS_FOLDER/test.py'][0m
[[34m2022-07-26 04:36:15,460[0m] {[34mdagbag.py:[0m508} INFO[0m - Filling up the DagBag from /Users/pro/Documents/proj_data/airflow/dags/test.py[0m
[[34m2022-07-26 04:36:15,511[0m] {[34mexample_local_kubernetes_executor.py:[0m37} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/Users/pro/Documents/a_env/proj_data/lib/python3.7/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2022-07-26 04:36:15,512[0m] {[34mexample_local_kubernetes_executor.py:[0m38} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:36:15,747[0m] {[34mexample_kubernetes_executor.py:[0m40} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2022-07-26 04:36:20,830[0m] {[34mtask_command.py:[0m371} INFO[0m - Running <TaskInstance: a_test.test_say_bye manual__2022-07-25T20:34:41.637079+00:00 [queued]> on host Fu-Changs-MacBook-Pro.local[0m
[[34m2022-07-26 04:36:41,246[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.sleep run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:36:41,247[0m] {[34mscheduler_job.py:[0m605} INFO[0m - Executor reports execution of a_test.test_say_bye run_id=manual__2022-07-25T20:34:41.637079+00:00 exited with status success for try_number 1[0m
[[34m2022-07-26 04:36:41,255[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=sleep, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:35:53.955741+00:00, run_end_date=2022-07-25 20:36:09.171016+00:00, run_duration=15.215275, state=success, executor_state=success, try_number=1, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-07-25 20:35:37.362955+00:00, queued_by_job_id=1, pid=32954[0m
[[34m2022-07-26 04:36:41,256[0m] {[34mscheduler_job.py:[0m662} INFO[0m - TaskInstance Finished: dag_id=a_test, task_id=test_say_bye, run_id=manual__2022-07-25T20:34:41.637079+00:00, map_index=-1, run_start_date=2022-07-25 20:36:30.870856+00:00, run_end_date=2022-07-25 20:36:41.024363+00:00, run_duration=10.153507, state=success, executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2022-07-25 20:35:37.362955+00:00, queued_by_job_id=1, pid=33038[0m
[[34m2022-07-26 04:36:41,267[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=32949) last sent a heartbeat 63.94 seconds ago! Restarting it[0m
[[34m2022-07-26 04:36:41,275[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 32949. PIDs of all processes in the group: [32949][0m
[[34m2022-07-26 04:36:41,275[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Sending the signal Signals.SIGTERM to group 32949[0m
[[34m2022-07-26 04:36:41,572[0m] {[34mprocess_utils.py:[0m75} INFO[0m - Process psutil.Process(pid=32949, status='terminated', exitcode=0, started='04:35:37') (32949) terminated with exit code 0[0m
[[34m2022-07-26 04:36:41,577[0m] {[34mmanager.py:[0m160} INFO[0m - Launched DagFileProcessorManager with pid: 33039[0m
[[34m2022-07-26 04:36:41,585[0m] {[34msettings.py:[0m55} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-07-26 04:36:41,605] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-07-26 04:36:41,789[0m] {[34mdagrun.py:[0m564} INFO[0m - Marking run <DagRun a_test @ 2022-07-25 20:34:41.637079+00:00: manual__2022-07-25T20:34:41.637079+00:00, externally triggered: True> successful[0m
[[34m2022-07-26 04:36:41,790[0m] {[34mdagrun.py:[0m624} INFO[0m - DagRun Finished: dag_id=a_test, execution_date=2022-07-25 20:34:41.637079+00:00, run_id=manual__2022-07-25T20:34:41.637079+00:00, run_start_date=2022-07-25 20:34:42.438960+00:00, run_end_date=2022-07-25 20:36:41.790469+00:00, run_duration=119.351509, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-07-24 20:34:41.637079+00:00, data_interval_end=2022-07-25 20:34:41.637079+00:00, dag_hash=1b9c2d33dcbd23233be2491803b3e455[0m
[[34m2022-07-26 04:36:41,793[0m] {[34mdag.py:[0m2972} INFO[0m - Setting next_dagrun for a_test to 2022-07-25T20:34:41.637079+00:00, run_after=2022-07-26T20:34:41.637079+00:00[0m
[[34m2022-07-26 04:37:21,473[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:42:21,576[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:47:21,726[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:52:21,865[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 04:57:22,017[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-07-26 05:02:22,159[0m] {[34mscheduler_job.py:[0m1233} INFO[0m - Resetting orphaned tasks for active dag runs[0m
